{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DOs:\n",
    "* attendere il feedback sulle news inviato per mail (14 agosto)\n",
    "* attendere la lista dei tags rivisti e rivedere poi il processo di assegnazione (13 agosto)\n",
    "* caricare tutto su airtable\n",
    "* Nel processo di update del database aggiungere l'opzione di dubbio e quindi segnalare startup da controllare manualmente\n",
    "* tenere il punto nella key\n",
    "* installare libreria postgres, collegarsi al databse con i dati che passa henri, porta 5432\n",
    "* pgadmin da browser, chiede sempre gli stessi dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARIES AND GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import pandas as pd\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from googlesearch import search\n",
    "import hashlib\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "\n",
    "# Initialize WebDriver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.binary_location = r'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe'\n",
    "chrome_options.add_argument('--headless')  # Run headless Chrome\n",
    "service = Service(r'C:\\Users\\kevin\\Desktop\\RetailHub\\new_software\\chromedriver\\chromedriver-win64\\chromedriver.exe')  # Update the path to your chromedriver\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "\n",
    "# Your Airtable API key and base ID\n",
    "api_key_airtable = 'patRHlt60PBL6jlV5.f2b9afed49bf23d9ab73cee6b283af76d3b4a9ebc0619002c6f104cc84382f2c'\n",
    "base_id = 'appUMz37tbS84AKX4'\n",
    "table_name = 'integrated_table'\n",
    "\n",
    "# Airtable API endpoint for the specified table\n",
    "url_airtable = f'https://api.airtable.com/v0/{base_id}/{table_name}'\n",
    "\n",
    "# Headers for the API request\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {api_key_airtable}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "\n",
    "# Carica le variabili d'ambiente dal file .env\n",
    "load_dotenv()\n",
    "\n",
    "# Valorizza la chiave API con la variabile dell'ambiente\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Configura il logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get all text from a given URL and its subpages\n",
    "def get_all_text(url, max_depth=2, current_depth=0, visited=None, only_main = False):\n",
    "    text = ''\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "        \n",
    "    if current_depth > max_depth or url in visited:\n",
    "        return \"\"\n",
    "\n",
    "    visited.add(url)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Remove script and style elements\n",
    "        for script in soup(['script', 'style']):\n",
    "            script.decompose()\n",
    "\n",
    "        # Get text from the current page\n",
    "        main_text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "        # ritorna solo il main\n",
    "        if only_main is True:\n",
    "            return main_text\n",
    "        \n",
    "        else:\n",
    "            text = main_text\n",
    "\n",
    "            # Find all links on the current page\n",
    "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "            \n",
    "            # Base URL to resolve relative links\n",
    "            base_url = requests.utils.urlparse(url)._replace(path=\"\", query=\"\", fragment=\"\").geturl()\n",
    "            \n",
    "            # Recursively get text from subpages\n",
    "            for link in links:\n",
    "                if not link.startswith('http'):\n",
    "                    link = requests.compat.urljoin(base_url, link)\n",
    "                text += \" \" + get_all_text(link, max_depth, current_depth + 1, visited)\n",
    "            \n",
    "            return text\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def analyze_text_with_gpt(text, initial_data):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, api_key=openai.api_key)\n",
    "\n",
    "    system_template = (\"You are a helpful assistant that extracts data from the user input.\")\n",
    "\n",
    "    template = '''You will be given all the text from all the website pages of one startup. \n",
    "                    Your task is to identify and return several pieces of information from the input text. \n",
    "                    Please format your answer (in English, mandatory) as follows (it MUST be a numeric pointed list):\n",
    "\n",
    "                    1. Name: The name of the brand/startup.\n",
    "                    2. Business_model: A couple of words regarding the business model of the startup.\n",
    "                    3. Business_description: At least 250 detailed words, in third person, with a title summarizing the business of the startup.\n",
    "                    4. Founding_year: The year the startup was founded.\n",
    "                    5. Founders: The names and roles of the founders.\n",
    "                    6. Product_description: A detailed description of the startup's main products or services, in third person, at least 250 words as bullet points.\n",
    "                    7. City: The city where the startup is headquartered.\n",
    "                    8. Country: The country where the startup is headquartered.\n",
    "                    9. Facebook_url: The startup's official Facebook page URL.\n",
    "                    10. Notable_achievements_awards: List any significant achievements or awards the startup has received.\n",
    "                    11. Target_markets: The target markets the startup aims to serve (europe, america, ecc....).\n",
    "                    12. Company_type: The type of company (e.g., LLC ecc...).\n",
    "                    13. Clients: List of notable clients.\n",
    "                    14. Tags: Relevant tags or keywords associated with the startup. Must be a subset of the followings: Advertising, Agricultural Tech, Agriculture, Analytics, APIs, AR/VR Technologies, Arts, Asset Management, Automation, Automotive Tech, B2B, Banking, Big Data, Blockchain, Business Development, Business Intelligence, Business Productivity, Cloud Computing, Communication Technology, Computer Vision, Consumer Electronics, Content Production, CRM, Customer Service, Customer Support, Cybersecurity, Data Privacy, Developer Tools, Digital Marketing, E-Commerce, E-Government, EdTech, Education Technology, Energy Tech, Entertainment, Environmental Tech, Event Management, Fashion, Fashion Tech, Finance, Financial Services, FinTech, Food and Beverage Industry, Gamification, Gaming, Geospatial Services, Green Tech, Health and Wellness, HealthTech, Home Improvement, Hospitality, Human Resources, Information Technology, Infrastructure Management, Innovation, Insurance, International, Investment, Learning, Legal Compliance, Localization, Logistics, Manufacturing, Marketing, Marketing Automation, Marketplaces, Materials Science, Media, Mobile Technology, Mobility Tech, Networking, Non-profit Sector, On-Demand Services, Outdoor, Payments, Personal Care, Personalization, Quality Assurance, Real Estate, Recruitment, Recycling, Research, Retail Technology, Revenue Models, Robotics, SaaS Solutions, Sales Growth, Security, SEO, Smart Home Tech, Social Impact, Social Networking, Software Development, Software Services, Sports Tech, Streaming Services, Internet of Things (IoT), Subscription Services, Supply Chain, Sustainability, Telecommunications, Travel, Veterinary Services, Waste Management, Wireless Tech, Workforce Management, CIRCULAR ECONOMY, Packaging, Shelf intelligence, Virtual Try-On, BNPL, Return Management, Visual Merchandising, Demand planning, Pricing Solution, size guide, Autonomous Store, Fraud prevention, Digital Checkout, workflow management, Rental Platform, Autonomous Store, SHELF MANAGEMENT, Eyewear Experience, shopping experience, ecommerce, 3D Technology, Accessibility Tech, Advanced Manufacturing, AI, NRF24, Digital Wallet, Retail Marketing, Customer Experience, Smart Shelf, In-Store Technology, Smart Tags, Task Management, SaaS Platform, Neuroscience Solutions, Sensory Marketing, Real-time Monitoring, AI Checkout, Fast Checkout, Seamless Checkout, Shelf Monitoring, Store Optimization, Physical Storage, Real Estate, Online Payments, Blockchain Integration, Neuroscience, Footwear Technology, digital transformation, Product Design, Real-Time Analytics, Robotics Solutions, Shelf Stocking, Data Automation, Real-time Solutions, CRM, Energy, Computer Vision, Trend Forecasting, Seamless Integration, Immersive Experiences, loyalty program, In-Video Checkout, Personalized Recipes, In-Store Insights, store locator, loyalty automation, product passport, In-store management, Click&Collect, Demand Forecasting, Checkout Alternative, crypto, On-Demand, Fleet Management, Facial Recognition, Gift Cards, Metaverse, In-store, Self-ordering, Buy Now Pay Later, smart planning, 3d commerce, dynamic inventory, pop-up, zero emission, waste, Pick Up, Picking, carbon neutrality, web 3.0, Customer acquisition, Free delivery, customer loyalty, data analytics, retail analytics, virtual fitting room, virtual try-on, store analytics, elearning, self-checkout, Pay later, Checkout Automation'.\n",
    "                    15. Phone_number: The contact phone number of the startup.\n",
    "                    16. Technologies_used: The areas of interest and technologies the startup utilizes.\n",
    "                    17. Address: The complete address of the startup.\n",
    "                    18. Region: The region where the startup is located (depends on the country).\n",
    "                    19. Number_of_employees: The total number of employees.\n",
    "                    20. Main_investors: The main investors in the startup.\n",
    "                    21. Number_of_investors: The total number of investors (based on the previous answer).\n",
    "                    22. Investment_funds: The investment funds involved with the startup.\n",
    "                    23. Exit_summary: A summary of the exit strategy or past exits.\n",
    "                    24. Total_funding: The total funding received by the startup (in currency).\n",
    "                    25. Advisors: List of advisors to the startup.\n",
    "                    26. LinkedIn_URL: The startup's official LinkedIn page URL.\n",
    "                    27. IPO_summary: Details about any IPOs.\n",
    "                    28. Value_of_the_startup: The valuation of the startup.\n",
    "                    29. Number_of_patents: The number of patents granted to the startup.\n",
    "                    30. Number_of_trademarks: The number of trademarks registered by the startup.\n",
    "                    31. Operating_status: startup, scaleup ecc.....\n",
    "                    32. Type_of_latest_investment: The type of the latest investment received.\n",
    "                    33. Acquired_by: The entity that acquired the startup, if applicable.\n",
    "                    34. Video_demo: URL to any video or demo available on the startup website.\n",
    "                    35. Website: The startup's official website URL.\n",
    "                    36. Revenue: the startup's annual revenue.\n",
    "                    37. Growth_rate: the growth rate of the last year for the startup.\n",
    "                    38. Logo_url: the url of the logo of the brand.\n",
    "\n",
    "                    If any of this information is not available in the text, try to figure it out on your own, otherwise write only the word NULL for that item and nothing else (mandatory).\n",
    "                    Additionally, you will be provided with a dictionary containing initial information about the startup. Please use this information as is and do not modify it. The dictionary is as follows: {initial_data}.\n",
    "                    The text to work on is the following: {data}.'''\n",
    "\n",
    "\n",
    "    # Create the chat prompt\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    user_prompt = PromptTemplate(template=template, input_variables=[\"data\"])\n",
    "    user_message_prompt = HumanMessagePromptTemplate(prompt=user_prompt)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, user_message_prompt])\n",
    "    llm_chain = LLMChain(prompt=chat_prompt, llm=llm, verbose=False)\n",
    "\n",
    "    result = llm_chain.run({\"data\": text, \"initial_data\": initial_data})\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def get_product_description(text):\n",
    "    start_keyword = \"6. Product description:\"\n",
    "    end_keyword = \"7.\"\n",
    "\n",
    "    start_index = text.find(start_keyword)\n",
    "    end_index = text.find(end_keyword)\n",
    "\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        # Extract and clean the product description\n",
    "        product_description = text[start_index + len(start_keyword):end_index].strip()\n",
    "        return product_description\n",
    "    else:\n",
    "        return \"NULL\"\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_dict(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Dictionary to hold the results\n",
    "    data_dict = {}\n",
    "    \n",
    "    # Regex to match the keys\n",
    "    key_pattern = re.compile(r'^\\d+\\. ([^:]+):')\n",
    "    \n",
    "    # Current key\n",
    "    current_key = None\n",
    "    \n",
    "    # Process each line\n",
    "    for line in lines:\n",
    "        key_match = key_pattern.match(line)\n",
    "        if key_match:\n",
    "            # Found a new key\n",
    "            current_key = key_match.group(1).strip()\n",
    "            value = line[key_match.end():].strip()\n",
    "            data_dict[current_key] = value\n",
    "        elif current_key:\n",
    "            # Append the line to the current key's value\n",
    "            data_dict[current_key] += ' ' + line.strip()\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "\n",
    "def is_json(response):\n",
    "    try:\n",
    "        json.loads(response.text)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to upload a single record to Airtable\n",
    "def upload_record_to_airtable(record, url, headers):\n",
    "    data = {\n",
    "    \"records\": [\n",
    "        {\n",
    "            \"fields\": {\n",
    "            \"Name\": record.get(\"Name\", \"NULL\"),\n",
    "            \"Business_model\": record.get(\"Business_model\", \"NULL\"),\n",
    "            \"Business_description\": record.get(\"Business_description\", \"NULL\"),\n",
    "            \"Founding_year\": record.get(\"Founding_year\", \"NULL\"),\n",
    "            \"Founders\": record.get(\"Founders\", \"NULL\"),\n",
    "            \"Product_description\": record.get(\"Product_description\", \"NULL\"),\n",
    "            \"City\": record.get(\"City\", \"NULL\"),\n",
    "            \"Country\": record.get(\"Country\", \"NULL\"),\n",
    "            \"Facebook_url\": record.get(\"Facebook_url\", \"NULL\"),\n",
    "            \"Notable_achievements_awards\": record.get(\"Notable_achievements_awards\", \"NULL\"),\n",
    "            \"Target_markets\": record.get(\"Target_markets\", \"NULL\"),\n",
    "            \"Company_type\": record.get(\"Company_type\", \"NULL\"),\n",
    "            \"Clients\": record.get(\"Clients\", \"NULL\"),\n",
    "            \"Tags\": record.get(\"Tags\", \"NULL\"),\n",
    "            \"Phone_number\": record.get(\"Phone_number\", \"NULL\"),\n",
    "            \"Technologies_used\": record.get(\"Technologies_used\", \"NULL\"),\n",
    "            \"Address\": record.get(\"Address\", \"NULL\"),\n",
    "            \"Region\": record.get(\"Region\", \"NULL\"),\n",
    "            \"Number_of_employees\": record.get(\"Number_of_employees\", \"NULL\"),\n",
    "            \"Main_investors\": record.get(\"Main_investors\", \"NULL\"),\n",
    "            \"Number_of_investors\": record.get(\"Number_of_investors\", \"NULL\"),\n",
    "            \"Investment_funds\": record.get(\"Investment_funds\", \"NULL\"),\n",
    "            \"Exit_summary\": record.get(\"Exit_summary\", \"NULL\"),\n",
    "            \"Total_funding\": record.get(\"Total_funding\", \"NULL\"),\n",
    "            \"Advisors\": record.get(\"Advisors\", \"NULL\"),\n",
    "            \"LinkedIn_URL\": record.get(\"LinkedIn_URL\", \"NULL\"),\n",
    "            \"IPO_summary\": record.get(\"IPO_summary\", \"NULL\"),\n",
    "            \"Value_of_the_startup\": record.get(\"Value_of_the_startup\", \"NULL\"),\n",
    "            \"Number_of_patents\": record.get(\"Number_of_patents\", \"NULL\"),\n",
    "            \"Number_of_trademarks\": record.get(\"Number_of_trademarks\", \"NULL\"),\n",
    "            \"Operating_status\": record.get(\"Operating_status\", \"NULL\"),\n",
    "            \"Type_of_latest_investment\": record.get(\"Type_of_latest_investment\", \"NULL\"),\n",
    "            \"Acquired_by\": record.get(\"Acquired_by\", \"NULL\"),\n",
    "            \"Video_demo\": record.get(\"Video_demo\", \"NULL\"),\n",
    "            \"Website\": record.get(\"Website\", \"NULL\"),\n",
    "            \"Revenue\": record.get(\"Revenue\", \"NULL\"),\n",
    "            \"Growth_rate\": record.get(\"Growth_rate\", \"NULL\"),\n",
    "            \"Logo_url\": record.get(\"Logo_url\", \"NULL\"),\n",
    "            \"Key\" : record.get(\"Key\", \"NULL\"),\n",
    "            \"google_news_urls\": record.get(\"google_news_urls\", \"NULL\")\n",
    "                    }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "}\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    logger.info(f\"Response status code: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        if is_json(response):\n",
    "            try:\n",
    "                response_json = response.json()\n",
    "                #logger.info(f\"Record added: {data['Name']}\")\n",
    "                #return response_json\n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(f\"Failed to parse response JSON: {e}\")\n",
    "                logger.error(f\"Response text: {response.text}\")\n",
    "        else:\n",
    "            logger.error(\"Response is not JSON.\")\n",
    "            logger.error(f\"Response text: {response.text}\")\n",
    "    else:\n",
    "        logger.error(f\"Failed to add record: {response.status_code}\")\n",
    "        logger.error(f\"Response text: {response.text}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "######## NEWS RETRIEVAL #########\n",
    "##################################\n",
    "\n",
    "\n",
    "def google_news_search(query, num_results=10):\n",
    "    try:\n",
    "        return list(search(query, num=num_results, stop=num_results, tbs='qdr:y'))  # 'tbs=qdr:y' filters for past year\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Google News search: {e}\")\n",
    "        return []\n",
    "\n",
    "def summarize_news_articles(articles_text):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, api_key=openai.api_key)\n",
    "\n",
    "    # Prompt template per il riassunto\n",
    "    template = '''\n",
    "    Qui di seguito c'è il testo di più articoli riguardanti una startup. Fornisci un riassunto conciso di solo 3 degli articoli riportati, separato da \";;;\":\n",
    "    Scegli attentamente quelli da riassumere senza ripeterti e ovviamente scegli articoli che possibilmente parlano di cose diverse.\n",
    "    Aggiungi il nome della fonte, la data di scrittura dell'articolo e chi l'ha scritto (se li trovi, se no non scrivere nulla).\n",
    "    {articles_text}\n",
    "    '''\n",
    "\n",
    "    # Creazione del prompt\n",
    "    user_prompt = PromptTemplate(template=template, input_variables=[\"articles_text\"])\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([HumanMessagePromptTemplate(prompt=user_prompt)])\n",
    "    llm_chain = LLMChain(prompt=chat_prompt, llm=llm, verbose=False)\n",
    "\n",
    "    # Chiamata all'API di GPT-4o-mini per ottenere il riassunto\n",
    "    result = llm_chain.run({\"articles_text\": articles_text})\n",
    "    \n",
    "    return result\n",
    "\n",
    "def news(startup_urls):\n",
    "    for startup_url in startup_urls:\n",
    "        print(f\"Processing {startup_url}...\")\n",
    "        \n",
    "        # Step 1: Search Google News for related articles \n",
    "        query = f\"{startup_url} startup news\"\n",
    "        search_results = google_news_search(query)\n",
    "        \n",
    "        if not search_results:\n",
    "            print(f\"No search results found for {startup_url}\")\n",
    "            continue\n",
    "\n",
    "        # Step 2: Scrape text from each found article\n",
    "        concatenated_text = \"\"\n",
    "        for result in search_results:\n",
    "            concatenated_text += get_all_text(result, only_main=True) + \" ---- END ---- \"\n",
    "\n",
    "        # Step 3: Generate summaries for the concatenated articles\n",
    "        summarized_text = summarize_news_articles(concatenated_text)\n",
    "    \n",
    "    return summarized_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "##################################\n",
    "# CREAZIONE DELLA CHIAVE UNIVOCA #\n",
    "##################################\n",
    "\n",
    "# Funzione per preprocessare la stringa\n",
    "def preprocess_string(s):\n",
    "    s = s.lower()  # Converti in minuscolo\n",
    "    s = re.sub(r'\\W+', '', s)  # Rimuovi tutti i caratteri non alfanumerici\n",
    "    return s\n",
    "\n",
    "# Funzione per normalizzare un URL\n",
    "def normalize_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    # Prendi solo il netloc (dominio) e il percorso (escludi http/https e www)\n",
    "    normalized_url = parsed_url.netloc.replace('www.', '') + parsed_url.path\n",
    "    # Rimuovi eventuali slash finali\n",
    "    normalized_url = normalized_url.rstrip('/')\n",
    "    return normalized_url\n",
    "\n",
    "# Funzione per generare una chiave univoca utilizzando solo il website\n",
    "def generate_unique_key(website):\n",
    "    website = preprocess_string(normalize_url(website))\n",
    "    print(website, website.encode())\n",
    "    unique_key = hashlib.sha256(website.encode()).hexdigest()\n",
    "    return unique_key\n",
    "\n",
    "\n",
    "#############################\n",
    "# ESTRAZIONE LINKS INTERESSANTI\n",
    "#############################\n",
    "def analyze_links_with_gpt(links):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, api_key=openai.api_key)\n",
    "\n",
    "    # Template per il prompt di sistema\n",
    "    system_template = (\n",
    "        \"Sei un assistente esperto che aiuta a identificare link web che contengono liste o ranking di startup. \"\n",
    "        \"Rispondi con una lista di link che parlano di liste o ranking di startup, uno per riga, senza numerazione o testo aggiuntivo.\"\n",
    "    )\n",
    "\n",
    "    # Template per il prompt dell'utente\n",
    "    template = '''\n",
    "    Ecco una lista di link web. Per favore, identifica quali di questi contengono liste o ranking di startup e restituisci solo i link:\n",
    "    {links}\n",
    "    '''\n",
    "\n",
    "    # Creazione del prompt\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    user_prompt = PromptTemplate(template=template, input_variables=[\"links\"])\n",
    "    user_message_prompt = HumanMessagePromptTemplate(prompt=user_prompt)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, user_message_prompt])\n",
    "    llm_chain = LLMChain(prompt=chat_prompt, llm=llm, verbose=False)\n",
    "\n",
    "    # Prepara i link come una stringa unica separata da nuovi linee\n",
    "    links_str = \"\\n\".join(links)\n",
    "\n",
    "    # Chiamata all'API di GPT-4 per ottenere il risultato filtrato\n",
    "    result = llm_chain.run({\"links\": links_str})\n",
    "    \n",
    "    # Post-processing: Rimuovi eventuali spazi vuoti e filtra solo le righe che contengono link validi\n",
    "    filtered_links = [link.strip() for link in result.split(\"\\n\") if link.strip().startswith(\"http\")]\n",
    "    \n",
    "    return filtered_links\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "# CREAZIONE DIZIONARIO STARTUP\n",
    "###############################\n",
    "\n",
    "dict_example = [\n",
    "                {'name': 'startup 1',\n",
    "                 'website': 'website 1',\n",
    "                 'venture_radar_profile': 'profile 1'},\n",
    "                {'name': 'startup 2',\n",
    "                 'website': 'website 2',\n",
    "                 'venture_radar_profile': 'profile 2'}\n",
    "                 ]\n",
    "\n",
    "# Funzione per analizzare il contenuto della pagina della startup\n",
    "def analyze_startup_page_with_gpt(html_content, dict_example):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, api_key=openai.api_key)\n",
    "\n",
    "    # Template per il prompt di sistema\n",
    "    system_template = (\n",
    "        \"Sei un assistente esperto che analizza il contenuto di una pagina web per estrarre le informazioni principali \"\n",
    "        \"relative a una startup, come il nome della startup, l'URL del sito della startup e l'URL del profilo della startup su Venture Radar. \"\n",
    "        \"Restituisci queste informazioni come lista python di dizionari, senza la formattazione markdown: rispondi solamente con quello richiesto.\"\n",
    "        \"il seguente è un esempio di output: {dict_example}\"\n",
    "        )\n",
    "\n",
    "    # Template per il prompt dell'utente\n",
    "    template = '''\n",
    "    Qui di seguito c'è il contenuto HTML di una pagina web che descrive una o più startup. \n",
    "    Estrai e restituisci le seguenti informazioni per ogni startup trovata, nel formato richiesto:\n",
    "    - Nome della startup\n",
    "    - URL del sito della startup\n",
    "    - URL del profilo della startup su Venture Radar\n",
    "    \n",
    "    Contenuto HTML della pagina:\n",
    "    {html_content}\n",
    "    '''\n",
    "\n",
    "    # Creazione del prompt\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    user_prompt = PromptTemplate(template=template, input_variables=[\"html_content\", \"dict_example\"])\n",
    "    user_message_prompt = HumanMessagePromptTemplate(prompt=user_prompt)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, user_message_prompt])\n",
    "    llm_chain = LLMChain(prompt=chat_prompt, llm=llm, verbose=False)\n",
    "\n",
    "    # Chiamata all'API di GPT-4 per ottenere il risultato analizzato\n",
    "    result = llm_chain.run({\"html_content\": html_content, \"dict_example\": dict_example})\n",
    "    \n",
    "    try:\n",
    "        # Tenta di caricare il risultato come JSON\n",
    "        startup_data = eval(result)\n",
    "        return startup_data\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to eval generated dict, returning raw result\")\n",
    "        print(\"Raw result:\", result)\n",
    "        return result  # Ritorna il risultato grezzo per ulteriori analisi\n",
    "\n",
    "\n",
    "##############################\n",
    "## LINK PROCESSING VENTURE\n",
    "##############################\n",
    "\n",
    "# Funzione per ottenere il link al sito web dalla pagina Venture Radar\n",
    "def get_website_from_ventureradar(link):\n",
    "    driver.get(link)\n",
    "    time.sleep(2)  # Attendi il caricamento della pagina\n",
    "    pageContent = driver.page_source\n",
    "    soup = BeautifulSoup(pageContent, 'html.parser')\n",
    "    website_div = soup.find('div', id='i_d_CompanyWebsiteLink')\n",
    "    if website_div and website_div.find('a', href=True):\n",
    "        return website_div.find('a')['href']\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def is_valid_url(url, expected_domain=None):\n",
    "    try:\n",
    "        result = re.match(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', url)\n",
    "        if expected_domain:\n",
    "            return result and expected_domain in url\n",
    "        return result is not None\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def process_startups(startup_data):\n",
    "    processed_startups = {}\n",
    "\n",
    "    # Itera su ogni startup nel dizionario\n",
    "    for startup in startup_data.get('page_1', []):\n",
    "        startup_name = startup.get('name')\n",
    "        startup_url = startup.get('website')\n",
    "        venture_radar_url = startup.get('venture_radar_profile')\n",
    "\n",
    "        # Caso 1: Sia il sito web che il sito interno sono valorizzati\n",
    "        if startup_url and venture_radar_url:\n",
    "            if not is_valid_url(venture_radar_url, \"ventureradar.com\") or startup_url == venture_radar_url:\n",
    "                # Se il sito interno non è valido o è uguale al sito web, ricostruiamo il sito interno\n",
    "                venture_radar_url = f\"https://www.ventureradar.com/organisation/{startup_name.replace(' ', '%20')}\"\n",
    "            processed_startups[startup_name] = {\n",
    "                \"Startup URL\": startup_url,\n",
    "                \"Venture Radar URL\": venture_radar_url\n",
    "            }\n",
    "\n",
    "        # Caso 2: Solo il sito interno è valorizzato\n",
    "        elif venture_radar_url:\n",
    "            if is_valid_url(venture_radar_url, \"ventureradar.com\"):\n",
    "                # Recuperiamo il sito web dalla pagina Venture Radar\n",
    "                startup_url = get_website_from_ventureradar(venture_radar_url)\n",
    "                if startup_url:\n",
    "                    processed_startups[startup_name] = {\n",
    "                        \"Startup URL\": startup_url,\n",
    "                        \"Venture Radar URL\": venture_radar_url\n",
    "                    }\n",
    "\n",
    "        # Caso 3: Solo il sito web è valorizzato\n",
    "        elif startup_url:\n",
    "            # Costruiamo il sito interno\n",
    "            venture_radar_url = f\"https://www.ventureradar.com/organisation/{startup_name.replace(' ', '%20')}\"\n",
    "            processed_startups[startup_name] = {\n",
    "                \"Startup URL\": startup_url,\n",
    "                \"Venture Radar URL\": venture_radar_url\n",
    "            }\n",
    "\n",
    "        # Caso 4: Nessuno dei due URL è valorizzato\n",
    "        else:\n",
    "            pass\n",
    "            # Non facciamo nulla, la startup non viene inclusa\n",
    "\n",
    "    return processed_startups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROWJO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL\n",
    "base_url2 = \"https://growjo.com/\"\n",
    "\n",
    "\n",
    "# Loop through pages from 2 to 195\n",
    "for idxx, page_num in enumerate(range(1, 2)):\n",
    "    page_url = f\"https://growjo.com/home/{page_num}\"\n",
    "    print('Scraped page:', idxx+1)\n",
    "\n",
    "    try:\n",
    "        # Fetch the page\n",
    "        driver.get(page_url)\n",
    "        pageContent = driver.page_source\n",
    "        soup = BeautifulSoup(pageContent, 'html.parser')\n",
    "\n",
    "        # Find the main exhibitors div\n",
    "        startup_containers = soup.find_all('tr', class_='jss33 jss35', role='checkbox', tabindex='-1')\n",
    "\n",
    "        # For each row in the table\n",
    "        for idxxx, row in enumerate(startup_containers):\n",
    "            print('scraping of startup number', 50 * (idxx) + (idxxx + 1) )\n",
    "\n",
    "            data = []\n",
    "            try:\n",
    "                # Extract ranking\n",
    "                ranking = row.find('div', class_='ranking-wrapper').text.strip() if row.find('div', class_='ranking-wrapper') else ''\n",
    "                \n",
    "                # Extract company name and link\n",
    "                company_cell = row.find_all('td', class_='text-start')[0]\n",
    "                company_name = company_cell.find('a', href=lambda x: x and x.startswith('/company/')).text.strip() if company_cell.find('a') else ''\n",
    "                print(company_name)\n",
    "                company_link = company_cell.find('a', href=lambda x: x and x.startswith('/company/'))['href'] \n",
    "                \n",
    "                # Extract location details\n",
    "                location_cell = row.find_all('td', class_='text-start')[1]\n",
    "                location = location_cell.find('a', class_='custom-anchor')['href'].split('/')[-1] if location_cell.find('a') else ''\n",
    "                \n",
    "                # Extract country\n",
    "                country = row.find_all('td', class_='text-start')[2].text.strip() if len(row.find_all('td', class_='text-start')) > 2 else ''\n",
    "                \n",
    "                # Extract funding details\n",
    "                funding = row.find_all('td', class_='text-start')[3].text.strip() if len(row.find_all('td', class_='text-start')) > 3 else ''\n",
    "                \n",
    "                # Extract industry\n",
    "                industry_cell = row.find_all('td', class_='text-start')[4]\n",
    "                industry = industry_cell.find('a').text.strip() if industry_cell.find('a') else ''\n",
    "                \n",
    "                # Extract employee count\n",
    "                employee_count = row.find_all('td', class_='text-start')[5].text.strip() if len(row.find_all('td', class_='text-start')) > 5 else ''\n",
    "                \n",
    "                # Extract revenue\n",
    "                revenue = row.find_all('td', class_='text-start')[6].text.strip() if len(row.find_all('td', class_='text-start')) > 6 else ''\n",
    "                \n",
    "                # Extract growth rate\n",
    "                growth_rate = row.find_all('td', class_='text-start')[7].text.strip() if len(row.find_all('td', class_='text-start')) > 7 else ''\n",
    "                \n",
    "                # Extract key person name and link\n",
    "                key_person_cell = row.find_all('td', class_='text-start')[8]\n",
    "                key_person = key_person_cell.find('a').text.strip() if key_person_cell.find('a') else ''\n",
    "                key_person_link = key_person_cell.find('a')['href'] if key_person_cell.find('a') else ''\n",
    "                \n",
    "                # Extract position\n",
    "                position = row.find_all('td', class_='text-start')[9].text.strip() if len(row.find_all('td', class_='text-start')) > 9 else ''\n",
    "                \n",
    "                # single startup page\n",
    "                url_startup = base_url2[:-1] + company_link\n",
    "                driver.get(url_startup)\n",
    "\n",
    "                # prendo tutto il testo nella pagina della startup\n",
    "                main_text = get_all_text(url_startup, only_main = True)\n",
    "\n",
    "                # Wait for the exhibitors list to be present\n",
    "                pageContent = driver.page_source\n",
    "\n",
    "                # Parse the main page with BeautifulSoup\n",
    "                soup = BeautifulSoup(pageContent, 'html.parser')\n",
    "\n",
    "                # Extract body\n",
    "                startup_body = soup.find('div', class_='tempCompany card-body')\n",
    "\n",
    "                # Logo extraction\n",
    "                rectangle = startup_body.find('div', id = 'revenue-financials')\n",
    "                logo_element = rectangle.find('img', src=lambda x: x and \"logo.clearbit.com\" in x)\n",
    "                logo_url = 'https:' + logo_element['src'] if logo_element else None\n",
    "                \n",
    "                # Extract company link\n",
    "                company_link_element = rectangle.find('a', href=True)\n",
    "                company_link = 'https:' + company_link_element['href'] if company_link_element else None\n",
    "                \n",
    "                # Employee extraction\n",
    "                employee_internal_links = [base_url2[:-1] + a['href'] for a in soup.find_all('a', href=lambda x: x and \"/employee/\" in x)]\n",
    "                employee_internal_links = ', '.join(employee_internal_links)\n",
    "\n",
    "                # Append data to the list\n",
    "                data.append([\n",
    "                            company_name, country, location, funding,\n",
    "                            industry, employee_count, revenue, growth_rate, key_person,\n",
    "                            key_person_link, position, logo_url, company_link\n",
    "                        ])\n",
    "\n",
    "                new_data = pd.DataFrame(data, columns=[\"Startup Name\", \"Country\", \"City\", \"Total funding\", \"Industry\", \n",
    "                                       \"Number of employees\", \"Revenue\", \"Growth Rate\", \"Key Person\", \n",
    "                                       \"Key Person Link\", \"Position\", \"Logo URL\", \"Website URL\"])\n",
    "\n",
    "                # GPT-4 WORK\n",
    "                initial_data = new_data.to_dict()\n",
    "                url = new_data['Website URL'][0]\n",
    "\n",
    "                text2 = get_all_text(url)\n",
    "                text2 += main_text\n",
    "                text_news = news([company_name])\n",
    "                text2 += text_news\n",
    "                print('get_all_text è andato OK')\n",
    "\n",
    "                analysis_result = analyze_text_with_gpt(text2, initial_data)\n",
    "                print('analyze_text_with_gpt è andato OK')\n",
    "\n",
    "                # Convert the analysis result into a dictionary\n",
    "                data_dict = convert_to_dict(analysis_result)\n",
    "                print('convert_to_dict è andato OK')\n",
    "\n",
    "                data_dict['google_news_urls'] = text_news\n",
    "\n",
    "                data_dict['Key'] = generate_unique_key(data_dict['Website'])\n",
    "                \n",
    "                # Convert the dictionary into a DataFrame\n",
    "                df = pd.DataFrame([data_dict])\n",
    "\n",
    "                # Upload the first record in 'df' to Airtable\n",
    "                upload_record_to_airtable(df.iloc[0].to_dict(), url_airtable, headers)\n",
    "                print('upload_record_to_airtable è andato OK')\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to scrape row: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape page {page_num}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VENTURE RADAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebDriver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.binary_location = r'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe'\n",
    "chrome_options.add_argument('--headless')  # Run headless Chrome\n",
    "service = Service(r'C:\\Users\\kevin\\Desktop\\RetailHub\\new_software\\chromedriver\\chromedriver-win64\\chromedriver.exe')  # Update the path to your chromedriver\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# URL di base\n",
    "base_url = \"https://blog.ventureradar.com/\"\n",
    "\n",
    "# Apri la pagina con Selenium\n",
    "driver.get(base_url)\n",
    "\n",
    "# Simula lo scroll fino in fondo alla pagina per caricare nuovi articoli\n",
    "SCROLL_PAUSE_TIME = 2\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scorri verso il basso\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Aspetta il caricamento della pagina\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Calcola l'altezza nuova della pagina dopo lo scroll\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # Esci dal ciclo se non ci sono nuovi contenuti da caricare\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Ottieni il contenuto completo della pagina dopo aver caricato tutti gli articoli\n",
    "pageContent = driver.page_source\n",
    "\n",
    "# Usa BeautifulSoup per analizzare il contenuto della pagina\n",
    "soup = BeautifulSoup(pageContent, 'html.parser')\n",
    "\n",
    "# Trova tutti gli oggetti di tipo <article>\n",
    "articles = soup.find_all('article')\n",
    "\n",
    "# Lista per memorizzare i link trovati\n",
    "links = []\n",
    "\n",
    "# Itera su ogni <article> per estrarre il link\n",
    "for article in articles:\n",
    "    link = article.find('a', href=True)\n",
    "    if link:\n",
    "        links.append(link['href'])\n",
    "\n",
    "filtered_links = analyze_links_with_gpt(links)\n",
    "\n",
    "\n",
    "# Dizionario per memorizzare le informazioni di tutte le startup\n",
    "startups_info = {}\n",
    "\n",
    "# Itera su ogni link filtrato\n",
    "for idx, link in enumerate(filtered_links):\n",
    "    print(f\"Processing link number: {idx+1} of {len(filtered_links)} total links\")\n",
    "    \n",
    "    # Apri la pagina del link con Selenium\n",
    "    driver.get(link)\n",
    "    time.sleep(3)  # Attendi il caricamento della pagina\n",
    "    \n",
    "    # Ottieni il contenuto della pagina\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "    # Estrai il contenuto del div con la classe \"entry-content\"\n",
    "    entry_content_div = soup.find('div', class_='entry-content')\n",
    "    \n",
    "    if entry_content_div:\n",
    "        # Converti il contenuto del div in una stringa HTML\n",
    "        html_content = str(entry_content_div)\n",
    "        \n",
    "        # Analizza il contenuto HTML della pagina per estrarre le informazioni delle startup\n",
    "        startup_data = analyze_startup_page_with_gpt(html_content, dict_example)\n",
    "        \n",
    "        # Aggiungi le informazioni al dizionario principale\n",
    "        startups_info[f\"page_{idx+1}\"] = startup_data\n",
    "    else:\n",
    "        print(f\"No entry-content found for link: {link}\")\n",
    "\n",
    "    break \n",
    "\n",
    "# Elaborazione dei dati delle startup\n",
    "processed_startups_info = process_startups(startups_info)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for startup_name, urls in processed_startups_info.items():\n",
    "        print(f\"Processing startup: {startup_name}\")\n",
    "        \n",
    "        # Ottieni il testo completo dal sito web della startup\n",
    "        startup_url = urls.get('Startup URL')\n",
    "        if startup_url:\n",
    "            startup_text = get_all_text(startup_url)\n",
    "        else:\n",
    "            startup_text = \"\"\n",
    "        \n",
    "        internal_startup_url = urls.get('Venture Radar URL')\n",
    "\n",
    "        if internal_startup_url:\n",
    "            #internal_startup_text = get_all_text(internal_startup_url, only_main=True)\n",
    "            driver.get(internal_startup_url)\n",
    "            time.sleep(3)  # Attendi il caricamento della pagina\n",
    "    \n",
    "            # Ottieni il contenuto della pagina\n",
    "            page_content = driver.page_source\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "            # Estrai il contenuto del div con la classe \"entry-content\"\n",
    "            internal_startup_text = soup.find('div', class_='container c_d_ProfileContainer')\n",
    "\n",
    "        else:\n",
    "            internal_startup_text = \"\"\n",
    "\n",
    "        # Ottieni le notizie relative alla startup\n",
    "        news_summary = news([startup_name])  \n",
    "\n",
    "        # Concatena le informazioni per l'analisi GPT\n",
    "        full_text = f\"Startup Information:\\n{startup_text}\\n\\nNews Summary:\\n{news_summary}\"\n",
    "        \n",
    "        # Analizza il testo concatenato usando GPT\n",
    "        gpt_analysis = analyze_text_with_gpt(full_text, initial_data = {internal_startup_text})\n",
    "        data_dict = convert_to_dict(gpt_analysis)\n",
    "        \n",
    "        data_dict['google_news_urls'] = news_summary\n",
    "        data_dict['Key'] = generate_unique_key(startup_url)\n",
    "\n",
    "        # Convert the dictionary into a DataFrame\n",
    "        df = pd.DataFrame([data_dict])\n",
    "\n",
    "        # Upload the first record in 'df' to Airtable\n",
    "        upload_record_to_airtable(df.iloc[0].to_dict(), url_airtable, headers)\n",
    "        print('upload_record_to_airtable è andato OK')\n",
    "        \n",
    "        break            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing startup: T-Therapeutics\n",
      "Failed to scrape javascript:void(0): No connection adapters were found for 'javascript:void(0)'\n",
      "Processing T-Therapeutics...\n",
      "Failed to scrape https://www.geekwire.com/tag/biotech/: 403 Client Error: Forbidden for url: https://www.geekwire.com/tag/biotech/\n",
      "ttherapeuticscom b'ttherapeuticscom'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Response status code: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload_record_to_airtable è andato OK\n"
     ]
    }
   ],
   "source": [
    "for startup_name, urls in processed_startups_info.items():\n",
    "        print(f\"Processing startup: {startup_name}\")\n",
    "        \n",
    "        # Ottieni il testo completo dal sito web della startup\n",
    "        startup_url = urls.get('Startup URL')\n",
    "        if startup_url:\n",
    "            startup_text = get_all_text(startup_url)\n",
    "        else:\n",
    "            startup_text = \"\"\n",
    "        \n",
    "        internal_startup_url = urls.get('Venture Radar URL')\n",
    "\n",
    "        if internal_startup_url:\n",
    "            #internal_startup_text = get_all_text(internal_startup_url, only_main=True)\n",
    "            driver.get(internal_startup_url)\n",
    "            time.sleep(3)  # Attendi il caricamento della pagina\n",
    "    \n",
    "            # Ottieni il contenuto della pagina\n",
    "            page_content = driver.page_source\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "            # Estrai il contenuto del div con la classe \"entry-content\"\n",
    "            internal_startup_text = soup.find('div', class_='container c_d_ProfileContainer')\n",
    "\n",
    "        else:\n",
    "            internal_startup_text = \"\"\n",
    "\n",
    "        # Ottieni le notizie relative alla startup\n",
    "        news_summary = news([startup_name])  \n",
    "\n",
    "        # Concatena le informazioni per l'analisi GPT\n",
    "        full_text = f\"Startup Information:\\n{startup_text}\\n\\nNews Summary:\\n{news_summary}\"\n",
    "        \n",
    "        # Analizza il testo concatenato usando GPT\n",
    "        gpt_analysis = analyze_text_with_gpt(full_text, initial_data = {internal_startup_text})\n",
    "        data_dict = convert_to_dict(gpt_analysis)\n",
    "        \n",
    "        data_dict['google_news_urls'] = news_summary\n",
    "        data_dict['Key'] = generate_unique_key(startup_url)\n",
    "\n",
    "        # Convert the dictionary into a DataFrame\n",
    "        df = pd.DataFrame([data_dict])\n",
    "\n",
    "        # Upload the first record in 'df' to Airtable\n",
    "        upload_record_to_airtable(df.iloc[0].to_dict(), url_airtable, headers)\n",
    "        print('upload_record_to_airtable è andato OK')\n",
    "        \n",
    "        break            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ttherapeuticscom b'ttherapeuticscom'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Response status code: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload_record_to_airtable è andato OK\n"
     ]
    }
   ],
   "source": [
    "data_dict = convert_to_dict(gpt_analysis)\n",
    "        \n",
    "data_dict['google_news_urls'] = news_summary\n",
    "data_dict['Key'] = generate_unique_key(startup_url)\n",
    "\n",
    "        # Convert the dictionary into a DataFrame\n",
    "df = pd.DataFrame([data_dict])\n",
    "\n",
    "# Upload the first record in 'df' to Airtable\n",
    "upload_record_to_airtable(df.iloc[0].to_dict(), url_airtable, headers)\n",
    "print('upload_record_to_airtable è andato OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['google_news_urls'] = news_summary\n",
    "data_dict['Key'] = generate_unique_key(data_dict['Website'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Name: T-Therapeutics\n",
      "2. Business_model: Biotechnology, Immuno-oncology\n",
      "3. Business_description: **Innovative T Cell Therapeutics for Cancer Treatment**  \n",
      "T-Therapeutics is a pioneering biotechnology company that specializes in developing next-generation T cell receptor (TCR) therapeutics aimed at transforming cancer treatment. Founded as a spin-off from the University of Cambridge, the company leverages advanced scientific research and technology to harness the natural power of T cells, a crucial component of the immune system, to target and destroy cancer cells. The company’s mission is to create safe and effective treatments for various cancers and autoimmune diseases by utilizing its proprietary OpTiMus® platform. This platform is a fully humanized TCR mouse model that generates unique human TCRs that do not exist in the human repertoire, allowing for the identification of optimal TCRs that can specifically recognize and attack cancer cells. T-Therapeutics is committed to reshaping the clinical landscape for cancer patients by developing a pipeline of first-in-class drugs that are designed to be transformative in their efficacy. The company combines expertise in mouse genome engineering, biopharmaceutical drug development, single-cell genomics, machine learning, and structural biology to create innovative therapies. With a culture rooted in creativity, collaboration, and scientific excellence, T-Therapeutics aims to bridge laboratory breakthroughs to clinical success, ultimately unlocking the immune system's potential for personalized cancer solutions.\n",
      "4. Founding_year: NULL\n",
      "5. Founders: Prof. Allan Bradley FRS, FMedSci - Founder and Chief Executive Officer\n",
      "6. Product_description: \n",
      "   - **OpTiMus® Platform**: A proprietary platform that utilizes a fully humanized TCR mouse to generate unique human TCRs for cancer therapy.\n",
      "   - **TCR-based Medicines**: Development of T cell receptor-based drugs that specifically target cancer cells by recognizing unique antigens.\n",
      "   - **T-Bridge® Molecules**: Precision medicines designed to target specific cancers based on individual genetic profiles and tissue-specific markers.\n",
      "   - **Transformative Cancer Therapies**: A pipeline of first-in-class drugs aimed at treating currently untreatable cancers, utilizing advanced genetic therapies and diagnostic tools.\n",
      "   - **Personalized Healthcare Solutions**: Leveraging molecular data to create individualized treatment plans for patients with cancer and autoimmune diseases.\n",
      "7. City: Cambridge\n",
      "8. Country: United Kingdom\n",
      "9. Facebook_url: NULL\n",
      "10. Notable_achievements_awards: NULL\n",
      "11. Target_markets: Europe, America\n",
      "12. Company_type: NULL\n",
      "13. Clients: NULL\n",
      "14. Tags: Biotechnology, HealthTech, Immuno-oncology, Cancer Treatment, T Cell Therapy\n",
      "15. Phone_number: NULL\n",
      "16. Technologies_used: T cell receptor technology, mouse genome engineering, machine learning, single-cell genomics, structural biology\n",
      "17. Address: Abington Hall Granta Park Cambridge CB21 6AL\n",
      "18. Region: NULL\n",
      "19. Number_of_employees: NULL\n",
      "20. Main_investors: Sofinnova Partners, F-Prime Capital, Digitalis Ventures, Cambridge Innovation Capital, Sanofi Ventures, University of Cambridge Venture Fund\n",
      "21. Number_of_investors: 6\n",
      "22. Investment_funds: NULL\n",
      "23. Exit_summary: NULL\n",
      "24. Total_funding: £48 million\n",
      "25. Advisors: NULL\n",
      "26. LinkedIn_URL: NULL\n",
      "27. IPO_summary: NULL\n",
      "28. Value_of_the_startup: NULL\n",
      "29. Number_of_patents: NULL\n",
      "30. Number_of_trademarks: NULL\n",
      "31. Operating_status: startup\n",
      "32. Type_of_latest_investment: Series A\n",
      "33. Acquired_by: NULL\n",
      "34. Video_demo: NULL\n",
      "35. Website: NULL\n",
      "36. Revenue: NULL\n",
      "37. Growth_rate: NULL\n",
      "38. Logo_url: NULL\n"
     ]
    }
   ],
   "source": [
    "print(gpt_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'startup_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(startup_data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'startup_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(startup_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK INTEGRATED_TABLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Airtable API Key e Base ID\n",
    "api_key = 'patRHlt60PBL6jlV5.f2b9afed49bf23d9ab73cee6b283af76d3b4a9ebc0619002c6f104cc84382f2c'\n",
    "base_id = 'appUMz37tbS84AKX4'\n",
    "table_name = 'integrated_table'\n",
    "\n",
    "# Endpoint dell'API di Airtable\n",
    "url = f'https://api.airtable.com/v0/{base_id}/{table_name}'\n",
    "\n",
    "\n",
    "# Intestazioni per la richiesta API\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {api_key}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "def get_all_records(url, headers):\n",
    "    all_records = []\n",
    "    params = {\"pageSize\": 100}\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        \n",
    "        records = response.json().get('records', [])\n",
    "        all_records.extend(records)\n",
    "        \n",
    "        # Verifica se c'è un'altra pagina di record\n",
    "        offset = response.json().get('offset')\n",
    "        if not offset:\n",
    "            break\n",
    "        params['offset'] = offset\n",
    "\n",
    "    return all_records\n",
    "\n",
    "# Ottieni tutti i record\n",
    "records = get_all_records(url, headers)\n",
    "\n",
    "# Converti i record in un DataFrame pandas per una visualizzazione migliore\n",
    "if records:\n",
    "    df = pd.DataFrame([record['fields'] for record in records])\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Nessun record trovato.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:42:03) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b36e66cb547cdc54b4925ce66c24e32c3ebe3f01179b2b2245a66e674bcdf804"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
