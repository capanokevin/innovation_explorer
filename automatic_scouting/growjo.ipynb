{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do:\n",
    "1. segnare i tag giusti nel prompt\n",
    "2. aggiungere il campo \"news\" con i link\n",
    "3. caricare tutto su airtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import pandas as pd\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from googlesearch import search\n",
    "import hashlib\n",
    "\n",
    "\n",
    "\n",
    "# Initialize WebDriver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.binary_location = r'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe'\n",
    "chrome_options.add_argument('--headless')  # Run headless Chrome\n",
    "service = Service(r'C:\\Users\\kevin\\Desktop\\RetailHub\\new_software\\chromedriver\\chromedriver-win64\\chromedriver.exe')  # Update the path to your chromedriver\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "\n",
    "# Your Airtable API key and base ID\n",
    "api_key_airtable = 'patRHlt60PBL6jlV5.f2b9afed49bf23d9ab73cee6b283af76d3b4a9ebc0619002c6f104cc84382f2c'\n",
    "base_id = 'appUMz37tbS84AKX4'\n",
    "table_name = 'integrated_table'\n",
    "\n",
    "# Base URL\n",
    "base_url2 = \"https://growjo.com/\"\n",
    "\n",
    "# Airtable API endpoint for the specified table\n",
    "url_airtable = f'https://api.airtable.com/v0/{base_id}/{table_name}'\n",
    "\n",
    "# Headers for the API request\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {api_key_airtable}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Your OpenAI API key\n",
    "openai.api_key = 'sk-64WRLuCUEs3SIYhuZa-yqWYNk_8DiAmRDLpeoW_pCvT3BlbkFJqnbKe2BxJyFAf70ErxSBVx79EkkxW4ZxnXaq0UpekA'\n",
    "\n",
    "# Configura il logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get all text from a given URL and its subpages\n",
    "def get_all_text(url, max_depth=2, current_depth=0, visited=None, only_main = False):\n",
    "    text = ''\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "        \n",
    "    if current_depth > max_depth or url in visited:\n",
    "        return \"\"\n",
    "\n",
    "    visited.add(url)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Remove script and style elements\n",
    "        for script in soup(['script', 'style']):\n",
    "            script.decompose()\n",
    "\n",
    "        # Get text from the current page\n",
    "        main_text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "        # ritorna solo il main\n",
    "        if only_main is True:\n",
    "            return main_text\n",
    "        \n",
    "        else:\n",
    "            text = main_text\n",
    "\n",
    "            # Find all links on the current page\n",
    "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "            \n",
    "            # Base URL to resolve relative links\n",
    "            base_url = requests.utils.urlparse(url)._replace(path=\"\", query=\"\", fragment=\"\").geturl()\n",
    "            \n",
    "            # Recursively get text from subpages\n",
    "            for link in links:\n",
    "                if not link.startswith('http'):\n",
    "                    link = requests.compat.urljoin(base_url, link)\n",
    "                text += \" \" + get_all_text(link, max_depth, current_depth + 1, visited)\n",
    "            \n",
    "            return text\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def analyze_text_with_gpt(text, initial_data):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, api_key=openai.api_key)\n",
    "\n",
    "    system_template = (\"You are a helpful assistant that extracts data from the user input.\")\n",
    "\n",
    "    template = '''You will be given all the text from all the website pages of one startup. \n",
    "                    Your task is to identify and return several pieces of information from the input text. \n",
    "                    Please format your answer (in English, mandatory) as follows (it MUST be a numeric pointed list):\n",
    "\n",
    "                    1. Name: The name of the brand/startup.\n",
    "                    2. Business_model: A couple of words regarding the business model of the startup.\n",
    "                    3. Business_description: At least 250 detailed words, in third person, with a title summarizing the business of the startup.\n",
    "                    4. Founding_year: The year the startup was founded.\n",
    "                    5. Founders: The names and roles of the founders.\n",
    "                    6. Product_description: A detailed description of the startup's main products or services, in third person, at least 250 words as bullet points.\n",
    "                    7. City: The city where the startup is headquartered.\n",
    "                    8. Country: The country where the startup is headquartered.\n",
    "                    9. Facebook_url: The startup's official Facebook page URL.\n",
    "                    10. Notable_achievements_awards: List any significant achievements or awards the startup has received.\n",
    "                    11. Target_markets: The target markets the startup aims to serve.\n",
    "                    12. Company_type: The type of company (e.g., private, public).\n",
    "                    13. Clients: List of notable clients.\n",
    "                    14. Tags: Relevant tags or keywords associated with the startup '(Advertising, Agricultural Tech, Agriculture, Analytics, APIs, AR/VR Technologies, Arts, Asset Management, Automation, Automotive Tech, B2B, Banking, Big Data, Blockchain, Business Development, Business Intelligence, Business Productivity, Cloud Computing, Communication Technology, Computer Vision, Consumer Electronics, Content Production, CRM, Customer Service, Customer Support, Cybersecurity, Data Privacy, Developer Tools, Digital Marketing, E-Commerce, E-Government, EdTech, Education Technology, Energy Tech, Entertainment, Environmental Tech, Event Management, Fashion, Fashion Tech, Finance, Financial Services, FinTech, Food and Beverage Industry, Gamification, Gaming, Geospatial Services, Green Tech, Health and Wellness, HealthTech, Home Improvement, Hospitality, Human Resources, Information Technology, Infrastructure Management, Innovation, Insurance, International, Investment, Learning, Legal Compliance, Localization, Logistics, Manufacturing, Marketing, Marketing Automation, Marketplaces, Materials Science, Media, Mobile Technology, Mobility Tech, Networking, Non-profit Sector, On-Demand Services, Outdoor, Payments, Personal Care, Personalization, Quality Assurance, Real Estate, Recruitment, Recycling, Research, Retail Technology, Revenue Models, Robotics, SaaS Solutions, Sales Growth, Security, SEO, Smart Home Tech, Social Impact, Social Networking, Software Development, Software Services, Sports Tech, Streaming Services, Internet of Things (IoT), Subscription Services, Supply Chain, Sustainability, Telecommunications, Travel, Veterinary Services, Waste Management, Wireless Tech, Workforce Management, CIRCULAR ECONOMY, Packaging, Shelf intelligence, Virtual Try-On, BNPL, Return Management, Visual Merchandising, Demand planning, Pricing Solution, size guide, Autonomous Store, Fraud prevention, Digital Checkout, workflow management, Rental Platform, Autonomous Store, SHELF MANAGEMENT, Eyewear Experience, shopping experience, ecommerce, 3D Technology, Accessibility Tech, Advanced Manufacturing, AI, NRF24, Digital Wallet, Retail Marketing, Customer Experience, Smart Shelf, In-Store Technology, Smart Tags, Task Management, SaaS Platform, Neuroscience Solutions, Sensory Marketing, Real-time Monitoring, AI Checkout, Fast Checkout, Seamless Checkout, Shelf Monitoring, Store Optimization, Physical Storage, Real Estate, Online Payments, Blockchain Integration, Neuroscience, Footwear Technology, digital transformation, Product Design, Real-Time Analytics, Robotics Solutions, Shelf Stocking, Data Automation, Real-time Solutions, CRM, Energy, Computer Vision, Trend Forecasting, Seamless Integration, Immersive Experiences, loyalty program, In-Video Checkout, Personalized Recipes, In-Store Insights, store locator, loyalty automation, product passport, In-store management, Click&Collect, Demand Forecasting, Checkout Alternative, crypto, On-Demand, Fleet Management, Facial Recognition, Gift Cards, Metaverse, In-store, Self-ordering, Buy Now Pay Later, smart planning, 3d commerce, dynamic inventory, pop-up, zero emission, waste, Pick Up, Picking, carbon neutrality, web 3.0, Customer acquisition, Free delivery, customer loyalty, data analytics, retail analytics, virtual fitting room, virtual try-on, store analytics, elearning, self-checkout, Pay later, Checkout Automation)'.\n",
    "                    15. Phone_number: The contact phone number of the startup.\n",
    "                    16. Technologies_used: The areas of interest and technologies the startup utilizes.\n",
    "                    17. Address: The complete address of the startup.\n",
    "                    18. Region: The region where the startup is located (depends on the country).\n",
    "                    19. Number_of_employees: The total number of employees.\n",
    "                    20. Main_investors: The main investors in the startup.\n",
    "                    21. Number_of_investors: The total number of investors.\n",
    "                    22. Investment_funds: The investment funds involved with the startup.\n",
    "                    23. Exit_summary: A summary of the exit strategy or past exits.\n",
    "                    24. Total_funding: The total funding received by the startup (in currency).\n",
    "                    25. Advisors: List of advisors to the startup.\n",
    "                    26. LinkedIn_URL: The startup's official LinkedIn page URL.\n",
    "                    27. IPO_summary: Details about any IPOs.\n",
    "                    28. Value_of_the_startup: The valuation of the startup.\n",
    "                    29. Number_of_patents: The number of patents granted to the startup.\n",
    "                    30. Number_of_trademarks: The number of trademarks registered by the startup.\n",
    "                    31. Operating_status: The current operating status of the startup (active or not).\n",
    "                    32. Type_of_latest_investment: The type of the latest investment received.\n",
    "                    33. Acquired_by: The entity that acquired the startup, if applicable.\n",
    "                    34. Video_demo: URL to any video or demo available.\n",
    "                    35. Website: The startup's official website URL.\n",
    "                    36. Revenue: the startup's annual revenue.\n",
    "                    37. Growth_rate: the growth rate of the last year for the startup.\n",
    "                    38. Logo_url: the url of the logo of the brand.\n",
    "\n",
    "                    If any of this information is not available in the text, write only the word NULL for that item and nothing else (mandatory).\n",
    "                    Additionally, you will be provided with a dictionary containing initial information about the startup. Please use this information as is and do not modify it. The dictionary is as follows: {initial_data}.\n",
    "                    The text to work on is the following: {data}.'''\n",
    "\n",
    "\n",
    "    # Create the chat prompt\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    user_prompt = PromptTemplate(template=template, input_variables=[\"data\"])\n",
    "    user_message_prompt = HumanMessagePromptTemplate(prompt=user_prompt)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, user_message_prompt])\n",
    "    llm_chain = LLMChain(prompt=chat_prompt, llm=llm, verbose=False)\n",
    "\n",
    "    result = llm_chain.run({\"data\": text, \"initial_data\": initial_data})\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def get_product_description(text):\n",
    "    start_keyword = \"6. Product description:\"\n",
    "    end_keyword = \"7.\"\n",
    "\n",
    "    start_index = text.find(start_keyword)\n",
    "    end_index = text.find(end_keyword)\n",
    "\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        # Extract and clean the product description\n",
    "        product_description = text[start_index + len(start_keyword):end_index].strip()\n",
    "        return product_description\n",
    "    else:\n",
    "        return \"NULL\"\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_dict(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Dictionary to hold the results\n",
    "    data_dict = {}\n",
    "    \n",
    "    # Regex to match the keys\n",
    "    key_pattern = re.compile(r'^\\d+\\. ([^:]+):')\n",
    "    \n",
    "    # Current key\n",
    "    current_key = None\n",
    "    \n",
    "    # Process each line\n",
    "    for line in lines:\n",
    "        key_match = key_pattern.match(line)\n",
    "        if key_match:\n",
    "            # Found a new key\n",
    "            current_key = key_match.group(1).strip()\n",
    "            value = line[key_match.end():].strip()\n",
    "            data_dict[current_key] = value\n",
    "        elif current_key:\n",
    "            # Append the line to the current key's value\n",
    "            data_dict[current_key] += ' ' + line.strip()\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "\n",
    "def is_json(response):\n",
    "    try:\n",
    "        json.loads(response.text)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to upload a single record to Airtable\n",
    "def upload_record_to_airtable(record, url, headers):\n",
    "    data = {\n",
    "    \"records\": [\n",
    "        {\n",
    "            \"fields\": {\n",
    "            \"Name\": record.get(\"Name\", \"NULL\"),\n",
    "            \"Business_model\": record.get(\"Business_model\", \"NULL\"),\n",
    "            \"Business_description\": record.get(\"Business_description\", \"NULL\"),\n",
    "            \"Founding_year\": record.get(\"Founding_year\", \"NULL\"),\n",
    "            \"Founders\": record.get(\"Founders\", \"NULL\"),\n",
    "            \"Product_description\": record.get(\"Product_description\", \"NULL\"),\n",
    "            \"City\": record.get(\"City\", \"NULL\"),\n",
    "            \"Country\": record.get(\"Country\", \"NULL\"),\n",
    "            \"Facebook_url\": record.get(\"Facebook_url\", \"NULL\"),\n",
    "            \"Notable_achievements_awards\": record.get(\"Notable_achievements_awards\", \"NULL\"),\n",
    "            \"Target_markets\": record.get(\"Target_markets\", \"NULL\"),\n",
    "            \"Company_type\": record.get(\"Company_type\", \"NULL\"),\n",
    "            \"Clients\": record.get(\"Clients\", \"NULL\"),\n",
    "            \"Tags\": record.get(\"Tags\", \"NULL\"),\n",
    "            \"Phone_number\": record.get(\"Phone_number\", \"NULL\"),\n",
    "            \"Technologies_used\": record.get(\"Technologies_used\", \"NULL\"),\n",
    "            \"Address\": record.get(\"Address\", \"NULL\"),\n",
    "            \"Region\": record.get(\"Region\", \"NULL\"),\n",
    "            \"Number_of_employees\": record.get(\"Number_of_employees\", \"NULL\"),\n",
    "            \"Main_investors\": record.get(\"Main_investors\", \"NULL\"),\n",
    "            \"Number_of_investors\": record.get(\"Number_of_investors\", \"NULL\"),\n",
    "            \"Investment_funds\": record.get(\"Investment_funds\", \"NULL\"),\n",
    "            \"Exit_summary\": record.get(\"Exit_summary\", \"NULL\"),\n",
    "            \"Total_funding\": record.get(\"Total_funding\", \"NULL\"),\n",
    "            \"Advisors\": record.get(\"Advisors\", \"NULL\"),\n",
    "            \"LinkedIn_URL\": record.get(\"LinkedIn_URL\", \"NULL\"),\n",
    "            \"IPO_summary\": record.get(\"IPO_summary\", \"NULL\"),\n",
    "            \"Value_of_the_startup\": record.get(\"Value_of_the_startup\", \"NULL\"),\n",
    "            \"Number_of_patents\": record.get(\"Number_of_patents\", \"NULL\"),\n",
    "            \"Number_of_trademarks\": record.get(\"Number_of_trademarks\", \"NULL\"),\n",
    "            \"Operating_status\": record.get(\"Operating_status\", \"NULL\"),\n",
    "            \"Type_of_latest_investment\": record.get(\"Type_of_latest_investment\", \"NULL\"),\n",
    "            \"Acquired_by\": record.get(\"Acquired_by\", \"NULL\"),\n",
    "            \"Video_demo\": record.get(\"Video_demo\", \"NULL\"),\n",
    "            \"Website\": record.get(\"Website\", \"NULL\"),\n",
    "            \"Revenue\": record.get(\"Revenue\", \"NULL\"),\n",
    "            \"Growth_rate\": record.get(\"Growth_rate\", \"NULL\"),\n",
    "            \"Logo_url\": record.get(\"Logo_url\", \"NULL\"),\n",
    "            \"Key\" : record.get(\"Key\", \"NULL\")\n",
    "                    }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "}\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    logger.info(f\"Response status code: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        if is_json(response):\n",
    "            try:\n",
    "                response_json = response.json()\n",
    "                #logger.info(f\"Record added: {data['Name']}\")\n",
    "                #return response_json\n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(f\"Failed to parse response JSON: {e}\")\n",
    "                logger.error(f\"Response text: {response.text}\")\n",
    "        else:\n",
    "            logger.error(\"Response is not JSON.\")\n",
    "            logger.error(f\"Response text: {response.text}\")\n",
    "    else:\n",
    "        logger.error(f\"Failed to add record: {response.status_code}\")\n",
    "        logger.error(f\"Response text: {response.text}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "######## NEWS RETRIEVAL #########\n",
    "##################################\n",
    "\n",
    "\n",
    "def google_news_search(query, num_results=5):\n",
    "    try:\n",
    "        return list(search(query, num=num_results, stop=num_results, tbs='qdr:y'))  # 'tbs=qdr:y' filters for past year\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Google News search: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def news(startup_urls):\n",
    "    \n",
    "    for startup_url in startup_urls:\n",
    "        print(f\"Processing {startup_url}...\")\n",
    "        \n",
    "        # Step 1: Search Google News for related articles \n",
    "        query = f\"{startup_url} startup news\"\n",
    "        search_results = google_news_search(query)\n",
    "        print(f\"Search results for {startup_url}: {search_results}\")  # Debugging line\n",
    "        \n",
    "        if not search_results:\n",
    "            print(f\"No search results found for {startup_url}\")\n",
    "            continue\n",
    "\n",
    "        # Step 2: Scrape text from each found article\n",
    "        concatenated_text = \"\"\n",
    "        for result in search_results:\n",
    "            concatenated_text += get_all_text(result, only_main=True) + \" ---- END ---- \"\n",
    "    \n",
    "    return concatenated_text\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "# CREAZIONE DELLA CHIAVE UNIVOCA #\n",
    "##################################\n",
    "\n",
    "def preprocess_string(s):\n",
    "    s = s.lower()  # Convert to lowercase\n",
    "    s = re.sub(r'\\W+', '', s)  # Remove all non-alphanumeric characters\n",
    "    return s\n",
    "\n",
    "def generate_unique_key(name, website):\n",
    "    name = preprocess_string(name)\n",
    "    website = preprocess_string(website)\n",
    "    combined_string = name + website\n",
    "    unique_key = hashlib.sha256(combined_string.encode()).hexdigest()\n",
    "    return unique_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped page: 1\n",
      "scraping of startup number 1\n",
      "Anthropic\n",
      "Failed to scrape mailto:press@anthropic.com: No connection adapters were found for 'mailto:press@anthropic.com'\n",
      "Processing Anthropic...\n",
      "Search results for Anthropic: ['https://www.theguardian.com/technology/article/2024/aug/08/anthropic-ai-startup-uk-regulator-cma-amazon-investment', 'https://techcrunch.com/2024/08/08/uk-launches-formal-probe-into-amazons-ties-with-ai-startup-anthropic/', 'https://www.wsj.com/tech/amazons-4-billion-investment-in-ai-startup-anthropic-comes-under-u-k-scrutiny-0fe5652e', 'https://www.morningstar.com/news/dow-jones/202408089721/amazons-4-billion-investment-in-ai-startup-anthropic-comes-under-uk-scrutiny-update', 'https://www.livemint.com/companies/news/amazons-4-billion-investment-in-ai-startup-anthropic-comes-under-uk-scrutiny-11723127184458.html']\n",
      "Failed to scrape https://www.wsj.com/tech/amazons-4-billion-investment-in-ai-startup-anthropic-comes-under-u-k-scrutiny-0fe5652e: 403 Client Error: Forbidden for url: https://www.wsj.com/tech/amazons-4-billion-investment-in-ai-startup-anthropic-comes-under-u-k-scrutiny-0fe5652e\n",
      "get_all_text è andato OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevin\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\kevin\\AppData\\Local\\r-miniconda\\envs\\r-reticulate\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze_text_with_gpt è andato OK\n",
      "convert_to_dict è andato OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Response status code: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload_record_to_airtable è andato OK\n"
     ]
    }
   ],
   "source": [
    "# Base URL\n",
    "base_url2 = \"https://growjo.com/\"\n",
    "\n",
    "\n",
    "# Loop through pages from 2 to 195\n",
    "for idxx, page_num in enumerate(range(1, 2)):\n",
    "    page_url = f\"https://growjo.com/home/{page_num}\"\n",
    "    print('Scraped page:', idxx+1)\n",
    "\n",
    "    try:\n",
    "        # Fetch the page\n",
    "        driver.get(page_url)\n",
    "        pageContent = driver.page_source\n",
    "        soup = BeautifulSoup(pageContent, 'html.parser')\n",
    "\n",
    "        # Find the main exhibitors div\n",
    "        startup_containers = soup.find_all('tr', class_='jss33 jss35', role='checkbox', tabindex='-1')\n",
    "\n",
    "        # For each row in the table\n",
    "        for idxxx, row in enumerate(startup_containers):\n",
    "            print('scraping of startup number', 50 * (idxx) + (idxxx + 1) )\n",
    "\n",
    "            data = []\n",
    "            try:\n",
    "                # Extract ranking\n",
    "                ranking = row.find('div', class_='ranking-wrapper').text.strip() if row.find('div', class_='ranking-wrapper') else ''\n",
    "                \n",
    "                # Extract company name and link\n",
    "                company_cell = row.find_all('td', class_='text-start')[0]\n",
    "                company_name = company_cell.find('a', href=lambda x: x and x.startswith('/company/')).text.strip() if company_cell.find('a') else ''\n",
    "                print(company_name)\n",
    "                company_link = company_cell.find('a', href=lambda x: x and x.startswith('/company/'))['href'] \n",
    "                \n",
    "                # Extract location details\n",
    "                location_cell = row.find_all('td', class_='text-start')[1]\n",
    "                location = location_cell.find('a', class_='custom-anchor')['href'].split('/')[-1] if location_cell.find('a') else ''\n",
    "                \n",
    "                # Extract country\n",
    "                country = row.find_all('td', class_='text-start')[2].text.strip() if len(row.find_all('td', class_='text-start')) > 2 else ''\n",
    "                \n",
    "                # Extract funding details\n",
    "                funding = row.find_all('td', class_='text-start')[3].text.strip() if len(row.find_all('td', class_='text-start')) > 3 else ''\n",
    "                \n",
    "                # Extract industry\n",
    "                industry_cell = row.find_all('td', class_='text-start')[4]\n",
    "                industry = industry_cell.find('a').text.strip() if industry_cell.find('a') else ''\n",
    "                \n",
    "                # Extract employee count\n",
    "                employee_count = row.find_all('td', class_='text-start')[5].text.strip() if len(row.find_all('td', class_='text-start')) > 5 else ''\n",
    "                \n",
    "                # Extract revenue\n",
    "                revenue = row.find_all('td', class_='text-start')[6].text.strip() if len(row.find_all('td', class_='text-start')) > 6 else ''\n",
    "                \n",
    "                # Extract growth rate\n",
    "                growth_rate = row.find_all('td', class_='text-start')[7].text.strip() if len(row.find_all('td', class_='text-start')) > 7 else ''\n",
    "                \n",
    "                # Extract key person name and link\n",
    "                key_person_cell = row.find_all('td', class_='text-start')[8]\n",
    "                key_person = key_person_cell.find('a').text.strip() if key_person_cell.find('a') else ''\n",
    "                key_person_link = key_person_cell.find('a')['href'] if key_person_cell.find('a') else ''\n",
    "                \n",
    "                # Extract position\n",
    "                position = row.find_all('td', class_='text-start')[9].text.strip() if len(row.find_all('td', class_='text-start')) > 9 else ''\n",
    "                \n",
    "                # single startup page\n",
    "                url_startup = base_url2[:-1] + company_link\n",
    "                driver.get(url_startup)\n",
    "\n",
    "                # prendo tutto il testo nella pagina della startup\n",
    "                main_text = get_all_text(url_startup, only_main = True)\n",
    "\n",
    "                # Wait for the exhibitors list to be present\n",
    "                pageContent = driver.page_source\n",
    "\n",
    "                # Parse the main page with BeautifulSoup\n",
    "                soup = BeautifulSoup(pageContent, 'html.parser')\n",
    "\n",
    "                # Extract body\n",
    "                startup_body = soup.find('div', class_='tempCompany card-body')\n",
    "\n",
    "                # Logo extraction\n",
    "                rectangle = startup_body.find('div', id = 'revenue-financials')\n",
    "                logo_element = rectangle.find('img', src=lambda x: x and \"logo.clearbit.com\" in x)\n",
    "                logo_url = 'https:' + logo_element['src'] if logo_element else None\n",
    "                \n",
    "                # Extract company link\n",
    "                company_link_element = rectangle.find('a', href=True)\n",
    "                company_link = 'https:' + company_link_element['href'] if company_link_element else None\n",
    "                \n",
    "                # Employee extraction\n",
    "                employee_internal_links = [base_url2[:-1] + a['href'] for a in soup.find_all('a', href=lambda x: x and \"/employee/\" in x)]\n",
    "                employee_internal_links = ', '.join(employee_internal_links)\n",
    "\n",
    "                # Append data to the list\n",
    "                data.append([\n",
    "                            company_name, country, location, funding,\n",
    "                            industry, employee_count, revenue, growth_rate, key_person,\n",
    "                            key_person_link, position, logo_url, company_link\n",
    "                        ])\n",
    "\n",
    "                new_data = pd.DataFrame(data, columns=[\"Startup Name\", \"Country\", \"City\", \"Total funding\", \"Industry\", \n",
    "                                       \"Number of employees\", \"Revenue\", \"Growth Rate\", \"Key Person\", \n",
    "                                       \"Key Person Link\", \"Position\", \"Logo URL\", \"Website URL\"])\n",
    "\n",
    "                # GPT-4 WORK\n",
    "                initial_data = new_data.to_dict()\n",
    "                url = new_data['Website URL'][0]\n",
    "\n",
    "                text2 = get_all_text(url)\n",
    "                text2 += main_text\n",
    "                text2 += news([company_name])\n",
    "                print('get_all_text è andato OK')\n",
    "\n",
    "                analysis_result = analyze_text_with_gpt(text2, initial_data)\n",
    "                print('analyze_text_with_gpt è andato OK')\n",
    "\n",
    "                # Convert the analysis result into a dictionary\n",
    "                data_dict = convert_to_dict(analysis_result)\n",
    "                print('convert_to_dict è andato OK')\n",
    "\n",
    "                data_dict['Key'] = generate_unique_key(data_dict['Name'], data_dict['Website'])\n",
    "                \n",
    "                # Convert the dictionary into a DataFrame\n",
    "                df = pd.DataFrame([data_dict])\n",
    "\n",
    "                # Upload the first record in 'df' to Airtable\n",
    "                upload_record_to_airtable(df.iloc[0].to_dict(), url_airtable, headers)\n",
    "                print('upload_record_to_airtable è andato OK')\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to scrape row: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape page {page_num}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anthropic'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://anthropic.com'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['Website']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['Key'] = generate_unique_key(data_dict['Name'], data_dict['Website'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 'Anthropic',\n",
       " 'Business_model': 'AI safety and research',\n",
       " 'Business_description': \"**AI Safety and Research Company** Anthropic is a pioneering AI safety and research company based in San Francisco, dedicated to developing reliable, interpretable, and steerable AI systems. Founded in 2021, the company has quickly established itself as a leader in the field of artificial intelligence, focusing on creating advanced AI models that prioritize safety and ethical considerations. The interdisciplinary team at Anthropic comprises experts from various domains, including machine learning, physics, policy, and product development. This diverse expertise enables them to generate cutting-edge research and develop AI systems that are not only powerful but also beneficial and trustworthy. Anthropic's flagship product, Claude, is a family of AI models designed to perform complex cognitive tasks, including advanced reasoning, vision analysis, and code generation. The company emphasizes the importance of safety in AI development, treating it as a systematic science. They conduct extensive research to understand the risks associated with AI and implement various safety techniques to mitigate these risks. Anthropic's commitment to transparency and collaboration with civil society, government, and academia further underscores its mission to ensure that AI technologies are developed responsibly and ethically. The company has garnered significant attention and investment, raising over $4.5 billion to date, and continues to push the boundaries of what is possible in AI while maintaining a strong focus on safety and societal impact.\",\n",
       " 'Founding_year': '2021',\n",
       " 'Founders': 'Jack Clark (Co-Founder), Tom Brown (Co-Founder), Sam McCandlish (Co-Founder), Jared Kaplan (Co-Founder), Dario Amodei (CEO and Co-Founder)',\n",
       " 'Product_description': ' - **Claude AI Models**: A family of AI models designed for various tasks, including: - **Claude 3.5 Sonnet**: The latest model that excels in advanced reasoning, outperforming competitors in evaluations. - **Claude API**: Allows businesses to integrate AI capabilities into their workflows, enhancing efficiency and productivity. - **Vision Analysis**: Capable of interpreting and analyzing images, including handwritten notes and complex graphs. - **Code Generation**: Assists in writing, debugging, and translating code, making it a valuable tool for developers. - **Multilingual Processing**: Supports real-time translation and content creation in multiple languages. - **Artifacts Feature**: A collaborative workspace for users to interact with AI-generated content in real-time. - **Safety Research**: Ongoing research initiatives focused on improving AI safety, including methods for scalable oversight and interpretability. - **Partnerships**: Collaborates with various organizations to promote safe AI practices and develop innovative solutions.',\n",
       " 'City': 'San Francisco',\n",
       " 'Country': 'USA',\n",
       " 'Facebook_url': 'NULL',\n",
       " 'Notable_achievements_awards': 'NULL',\n",
       " 'Target_markets': 'Businesses in various sectors including finance, healthcare, legal, and technology.',\n",
       " 'Company_type': 'Public Benefit Corporation (PBC)',\n",
       " 'Clients': 'Notable clients include Asana, Bridgewater Associates, LexisNexis, and Slack.',\n",
       " 'Tags': 'AI, machine learning, safety, research, technology, cloud computing.',\n",
       " 'Phone_number': 'NULL',\n",
       " 'Technologies_used': 'Machine learning, natural language processing, computer vision, cloud computing.',\n",
       " 'Address': '548 Market Street, PMB 90375, San Francisco, CA 94104, USA',\n",
       " 'Region': 'California',\n",
       " 'Number_of_employees': '573',\n",
       " 'Main_investors': 'Amazon, Google, Menlo Ventures.',\n",
       " 'Number_of_investors': 'NULL',\n",
       " 'Investment_funds': 'NULL',\n",
       " 'Exit_summary': 'NULL',\n",
       " 'Total_funding': '$4.51B',\n",
       " 'Advisors': 'NULL',\n",
       " 'LinkedIn_URL': 'NULL',\n",
       " 'IPO_summary': 'NULL',\n",
       " 'Value_of_the_startup': '$20B',\n",
       " 'Number_of_patents': 'NULL',\n",
       " 'Number_of_trademarks': 'NULL',\n",
       " 'Operating_status': 'Active',\n",
       " 'Type_of_latest_investment': 'Series C funding',\n",
       " 'Acquired_by': 'NULL',\n",
       " 'Video_demo': 'NULL',\n",
       " 'Website': 'https://anthropic.com',\n",
       " 'Revenue': '$721.9M',\n",
       " 'Growth_rate': '323%',\n",
       " 'Logo_url': 'https://logo.clearbit.com/anthropic.com?size=100',\n",
       " 'Key': 'e0791ce5ccc450d91eaa44ae1cd15e8a5b20715e595a649cdfdabd914ae37cd9'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check airtable principal table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Airtable API Key e Base ID\n",
    "api_key = 'patRHlt60PBL6jlV5.f2b9afed49bf23d9ab73cee6b283af76d3b4a9ebc0619002c6f104cc84382f2c'\n",
    "base_id = 'appUMz37tbS84AKX4'\n",
    "table_name = 'integrated_table'\n",
    "\n",
    "# Endpoint dell'API di Airtable\n",
    "url = f'https://api.airtable.com/v0/{base_id}/{table_name}'\n",
    "\n",
    "\n",
    "# Intestazioni per la richiesta API\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {api_key}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "def get_all_records(url, headers):\n",
    "    all_records = []\n",
    "    params = {\"pageSize\": 100}\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        \n",
    "        records = response.json().get('records', [])\n",
    "        all_records.extend(records)\n",
    "        \n",
    "        # Verifica se c'è un'altra pagina di record\n",
    "        offset = response.json().get('offset')\n",
    "        if not offset:\n",
    "            break\n",
    "        params['offset'] = offset\n",
    "\n",
    "    return all_records\n",
    "\n",
    "# Ottieni tutti i record\n",
    "records = get_all_records(url, headers)\n",
    "\n",
    "# Converti i record in un DataFrame pandas per una visualizzazione migliore\n",
    "if records:\n",
    "    df = pd.DataFrame([record['fields'] for record in records])\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Nessun record trovato.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b36e66cb547cdc54b4925ce66c24e32c3ebe3f01179b2b2245a66e674bcdf804"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
