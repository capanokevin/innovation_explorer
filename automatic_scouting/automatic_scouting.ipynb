{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DOs:\n",
    "* aggiungere il controllo sul website http ovunque\n",
    "* attendere il feedback sulle news inviato per mail (14 agosto)\n",
    "* attendere la lista dei tags rivisti e rivedere poi il processo di assegnazione (13 agosto)\n",
    "* caricare tutto su postgre \n",
    "* Nel processo di update del database aggiungere l'opzione di dubbio e quindi segnalare startup da controllare manualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARIES AND GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import pandas as pd\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from googlesearch import search\n",
    "import hashlib\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import MoveTargetOutOfBoundsException, ElementNotInteractableException, StaleElementReferenceException\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# Initialize WebDriver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.binary_location = r'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe'\n",
    "chrome_options.add_argument('--headless')  # Run headless Chrome\n",
    "service = Service(r'C:\\Users\\kevin\\Desktop\\RetailHub\\new_software\\chromedriver\\chromedriver-win64\\chromedriver.exe')  # Update the path to your chromedriver\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "\n",
    "# Your Airtable API key and base ID\n",
    "api_key_airtable = 'patRHlt60PBL6jlV5.f2b9afed49bf23d9ab73cee6b283af76d3b4a9ebc0619002c6f104cc84382f2c'\n",
    "base_id = 'appUMz37tbS84AKX4'\n",
    "table_name = 'integrated_table'\n",
    "\n",
    "# Airtable API endpoint for the specified table\n",
    "url_airtable = f'https://api.airtable.com/v0/{base_id}/{table_name}'\n",
    "\n",
    "# Headers for the API request\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {api_key_airtable}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Carica le variabili d'ambiente dal file .env\n",
    "load_dotenv()\n",
    "\n",
    "# Valorizza la chiave API con la variabile dell'ambiente\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Configura il logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) Gecko/20100101 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (iPad; CPU OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Android 11; Mobile; rv:89.0) Gecko/89.0 Firefox/89.0\",\n",
    "    \"Mozilla/5.0 (Linux; Android 11; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Mobile Safari/537.36\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_driver():\n",
    "    # Reinizializza il WebDriver\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.binary_location = r'C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe'\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Ensure GUI is off\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "\n",
    "    service = Service(r'C:\\Users\\kevin\\Desktop\\RetailHub\\new_software\\chromedriver\\chromedriver-win64\\chromedriver.exe')  # Aggiorna il percorso al tuo chromedriver\n",
    "    \n",
    "    # Restituisci il nuovo driver\n",
    "    return webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "\n",
    "def initialize_driver(headless=True, window_size=(1920, 1080)):\n",
    "    \"\"\"Inizializza e restituisce un driver configurato con opzioni avanzate.\"\"\"\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    \n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")  # ModalitÃ  headless opzionale\n",
    "    \n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-infobars\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    \n",
    "    # Seleziona un User-Agent casuale\n",
    "    user_agent = random.choice(USER_AGENTS)\n",
    "    chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "    \n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    \n",
    "    # Inizializza il WebDriver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    # Imposta la dimensione della finestra\n",
    "    driver.set_window_size(*window_size)\n",
    "    \n",
    "    # Esegui script per mascherare l'uso di webdriver\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    return driver\n",
    "    \n",
    "\n",
    "\n",
    "def get_all_text(url, max_depth=2, current_depth=0, visited=None, only_main=False, max_chars=50000):\n",
    "    \n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        # Se manca lo schema, aggiunge \"https://\"\n",
    "        url = 'https://' + url\n",
    "\n",
    "    text = ''\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    if current_depth > max_depth or url in visited:\n",
    "        return \"\"\n",
    "\n",
    "    visited.add(url)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Remove script and style elements\n",
    "        for script in soup(['script', 'style']):\n",
    "            script.decompose()\n",
    "\n",
    "        # Get text from the current page\n",
    "        main_text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "        # ritorna solo il main\n",
    "        if only_main is True:\n",
    "            return main_text\n",
    "        else:\n",
    "            text = main_text\n",
    "\n",
    "            # Check if the text has reached the limit\n",
    "            if len(text) >= max_chars:\n",
    "                return text[:max_chars]\n",
    "\n",
    "            # Find all links on the current page\n",
    "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "\n",
    "            # Base URL to resolve relative links\n",
    "            base_url = requests.utils.urlparse(url)._replace(path=\"\", query=\"\", fragment=\"\").geturl()\n",
    "\n",
    "            # Recursively get text from subpages\n",
    "            for link in links:\n",
    "                if not link.startswith('http'):\n",
    "                    link = requests.compat.urljoin(base_url, link)\n",
    "                \n",
    "                sub_text = get_all_text(link, max_depth, current_depth + 1, visited)\n",
    "                \n",
    "                # Append the subpage text if within the limit\n",
    "                if len(text) + len(sub_text) > max_chars:\n",
    "                    text += sub_text[:max_chars - len(text)]\n",
    "                    break\n",
    "                else:\n",
    "                    text += \" \" + sub_text\n",
    "\n",
    "            return text[:max_chars]\n",
    "\n",
    "    except requests.RequestException:\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "def analyze_text_with_gpt(text, initial_data):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, api_key=openai.api_key)\n",
    "\n",
    "    system_template = (\"You are a helpful assistant that extracts data from the user input.\")\n",
    "\n",
    "    template = '''You will be given all the text from all the website pages of one startup. \n",
    "                    Your task is to identify and return several pieces of information from the input text. \n",
    "                    Please format your answer (in English, mandatory) as follows (it MUST be a numeric pointed list):\n",
    "\n",
    "                    1. Name: The name of the brand/startup.\n",
    "                    2. Business_model: A couple of words regarding the business model of the startup.\n",
    "                    3. Business_description: At least 250 detailed words, in third person, with a title summarizing the business of the startup.\n",
    "                    4. Founding_year: The year the startup was founded.\n",
    "                    5. Founders: The names and roles of the founders.\n",
    "                    6. Product_description: A detailed description of the startup's main products or services, in third person, at least 250 words as bullet points.\n",
    "                    7. City: The city where the startup is headquartered.\n",
    "                    8. Country: The country where the startup is headquartered.\n",
    "                    9. Facebook_url: The startup's official Facebook page URL.\n",
    "                    10. Notable_achievements_awards: List any significant achievements or awards the startup has received.\n",
    "                    11. Target_markets: The target markets the startup aims to serve (europe, america, ecc....).\n",
    "                    12. Company_type: The type of company (e.g., LLC ecc...).\n",
    "                    13. Clients: List of notable clients.\n",
    "                    14. Tags: Relevant tags or keywords associated with the startup. Must be a subset of the followings: Advertising, Agricultural Tech, Agriculture, Analytics, APIs, AR/VR Technologies, Arts, Asset Management, Automation, Automotive Tech, B2B, Banking, Big Data, Blockchain, Business Development, Business Intelligence, Business Productivity, Cloud Computing, Communication Technology, Computer Vision, Consumer Electronics, Content Production, CRM, Customer Service, Customer Support, Cybersecurity, Data Privacy, Developer Tools, Digital Marketing, E-Commerce, E-Government, EdTech, Education Technology, Energy Tech, Entertainment, Environmental Tech, Event Management, Fashion, Fashion Tech, Finance, Financial Services, FinTech, Food and Beverage Industry, Gamification, Gaming, Geospatial Services, Green Tech, Health and Wellness, HealthTech, Home Improvement, Hospitality, Human Resources, Information Technology, Infrastructure Management, Innovation, Insurance, International, Investment, Learning, Legal Compliance, Localization, Logistics, Manufacturing, Marketing, Marketing Automation, Marketplaces, Materials Science, Media, Mobile Technology, Mobility Tech, Networking, Non-profit Sector, On-Demand Services, Outdoor, Payments, Personal Care, Personalization, Quality Assurance, Real Estate, Recruitment, Recycling, Research, Retail Technology, Revenue Models, Robotics, SaaS Solutions, Sales Growth, Security, SEO, Smart Home Tech, Social Impact, Social Networking, Software Development, Software Services, Sports Tech, Streaming Services, Internet of Things (IoT), Subscription Services, Supply Chain, Sustainability, Telecommunications, Travel, Veterinary Services, Waste Management, Wireless Tech, Workforce Management, CIRCULAR ECONOMY, Packaging, Shelf intelligence, Virtual Try-On, BNPL, Return Management, Visual Merchandising, Demand planning, Pricing Solution, size guide, Autonomous Store, Fraud prevention, Digital Checkout, workflow management, Rental Platform, Autonomous Store, SHELF MANAGEMENT, Eyewear Experience, shopping experience, ecommerce, 3D Technology, Accessibility Tech, Advanced Manufacturing, AI, NRF24, Digital Wallet, Retail Marketing, Customer Experience, Smart Shelf, In-Store Technology, Smart Tags, Task Management, SaaS Platform, Neuroscience Solutions, Sensory Marketing, Real-time Monitoring, AI Checkout, Fast Checkout, Seamless Checkout, Shelf Monitoring, Store Optimization, Physical Storage, Real Estate, Online Payments, Blockchain Integration, Neuroscience, Footwear Technology, digital transformation, Product Design, Real-Time Analytics, Robotics Solutions, Shelf Stocking, Data Automation, Real-time Solutions, CRM, Energy, Computer Vision, Trend Forecasting, Seamless Integration, Immersive Experiences, loyalty program, In-Video Checkout, Personalized Recipes, In-Store Insights, store locator, loyalty automation, product passport, In-store management, Click&Collect, Demand Forecasting, Checkout Alternative, crypto, On-Demand, Fleet Management, Facial Recognition, Gift Cards, Metaverse, In-store, Self-ordering, Buy Now Pay Later, smart planning, 3d commerce, dynamic inventory, pop-up, zero emission, waste, Pick Up, Picking, carbon neutrality, web 3.0, Customer acquisition, Free delivery, customer loyalty, data analytics, retail analytics, virtual fitting room, virtual try-on, store analytics, elearning, self-checkout, Pay later, Checkout Automation'.\n",
    "                    15. Phone_number: The contact phone number of the startup.\n",
    "                    16. Technologies_used: The areas of interest and technologies the startup utilizes.\n",
    "                    17. Address: The complete address of the startup.\n",
    "                    18. Region: The region where the startup is located (depends on the country).\n",
    "                    19. Number_of_employees: The total number of employees.\n",
    "                    20. Main_investors: The main investors in the startup.\n",
    "                    21. Number_of_investors: The total number of investors (based on the previous answer).\n",
    "                    22. Investment_funds: The investment funds involved with the startup.\n",
    "                    23. Exit_summary: A summary of the exit strategy or past exits.\n",
    "                    24. Total_funding: The total funding received by the startup (in currency).\n",
    "                    25. Advisors: List of advisors to the startup.\n",
    "                    26. LinkedIn_URL: The startup's official LinkedIn page URL.\n",
    "                    27. IPO_summary: Details about any IPOs.\n",
    "                    28. Value_of_the_startup: The valuation of the startup.\n",
    "                    29. Number_of_patents: The number of patents granted to the startup.\n",
    "                    30. Number_of_trademarks: The number of trademarks registered by the startup.\n",
    "                    31. Operating_status: startup, scaleup ecc.....\n",
    "                    32. Type_of_latest_investment: The type of the latest investment received.\n",
    "                    33. Acquired_by: The entity that acquired the startup, if applicable.\n",
    "                    34. Video_demo: URL to any video or demo available on the startup website.\n",
    "                    35. Website: The startup's official website URL.\n",
    "                    36. Revenue: the startup's annual revenue.\n",
    "                    37. Growth_rate: the growth rate of the last year for the startup.\n",
    "                    38. Logo_url: the url of the logo of the brand.\n",
    "\n",
    "                    If any of this information is not available in the text, try to figure it out on your own, otherwise write only the word NULL for that item and nothing else (mandatory).\n",
    "                    Additionally, you will be provided with a dictionary containing initial information about the startup. Please use this information as is and do not modify it. The dictionary is as follows: {initial_data}.\n",
    "                    The text to work on is the following: {data}.'''\n",
    "\n",
    "\n",
    "    # Create the chat prompt\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    user_prompt = PromptTemplate(template=template, input_variables=[\"data\"])\n",
    "    user_message_prompt = HumanMessagePromptTemplate(prompt=user_prompt)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, user_message_prompt])\n",
    "    llm_chain = LLMChain(prompt=chat_prompt, llm=llm, verbose=False)\n",
    "\n",
    "    result = llm_chain.run({\"data\": text, \"initial_data\": initial_data})\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def get_product_description(text):\n",
    "    start_keyword = \"6. Product description:\"\n",
    "    end_keyword = \"7.\"\n",
    "\n",
    "    start_index = text.find(start_keyword)\n",
    "    end_index = text.find(end_keyword)\n",
    "\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        # Extract and clean the product description\n",
    "        product_description = text[start_index + len(start_keyword):end_index].strip()\n",
    "        return product_description\n",
    "    else:\n",
    "        return \"NULL\"\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_dict(text):\n",
    "    # Split the text into lines\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Dictionary to hold the results\n",
    "    data_dict = {}\n",
    "    \n",
    "    # Regex to match the keys\n",
    "    key_pattern = re.compile(r'^\\d+\\. ([^:]+):')\n",
    "    \n",
    "    # Current key\n",
    "    current_key = None\n",
    "    \n",
    "    # Process each line\n",
    "    for line in lines:\n",
    "        key_match = key_pattern.match(line)\n",
    "        if key_match:\n",
    "            # Found a new key\n",
    "            current_key = key_match.group(1).strip()\n",
    "            value = line[key_match.end():].strip()\n",
    "            data_dict[current_key] = value\n",
    "        elif current_key:\n",
    "            # Append the line to the current key's value\n",
    "            data_dict[current_key] += ' ' + line.strip()\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "\n",
    "def is_json(response):\n",
    "    try:\n",
    "        json.loads(response.text)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to upload a single record to Airtable\n",
    "def upload_record_to_airtable(record, url, headers):\n",
    "    data = {\n",
    "    \"records\": [\n",
    "        {\n",
    "            \"fields\": {\n",
    "            \"Name\": record.get(\"Name\", \"NULL\"),\n",
    "            \"Business_model\": record.get(\"Business_model\", \"NULL\"),\n",
    "            \"Business_description\": record.get(\"Business_description\", \"NULL\"),\n",
    "            \"Founding_year\": record.get(\"Founding_year\", \"NULL\"),\n",
    "            \"Founders\": record.get(\"Founders\", \"NULL\"),\n",
    "            \"Product_description\": record.get(\"Product_description\", \"NULL\"),\n",
    "            \"City\": record.get(\"City\", \"NULL\"),\n",
    "            \"Country\": record.get(\"Country\", \"NULL\"),\n",
    "            \"Facebook_url\": record.get(\"Facebook_url\", \"NULL\"),\n",
    "            \"Notable_achievements_awards\": record.get(\"Notable_achievements_awards\", \"NULL\"),\n",
    "            \"Target_markets\": record.get(\"Target_markets\", \"NULL\"),\n",
    "            \"Company_type\": record.get(\"Company_type\", \"NULL\"),\n",
    "            \"Clients\": record.get(\"Clients\", \"NULL\"),\n",
    "            \"Tags\": record.get(\"Tags\", \"NULL\"),\n",
    "            \"Phone_number\": record.get(\"Phone_number\", \"NULL\"),\n",
    "            \"Technologies_used\": record.get(\"Technologies_used\", \"NULL\"),\n",
    "            \"Address\": record.get(\"Address\", \"NULL\"),\n",
    "            \"Region\": record.get(\"Region\", \"NULL\"),\n",
    "            \"Number_of_employees\": record.get(\"Number_of_employees\", \"NULL\"),\n",
    "            \"Main_investors\": record.get(\"Main_investors\", \"NULL\"),\n",
    "            \"Number_of_investors\": record.get(\"Number_of_investors\", \"NULL\"),\n",
    "            \"Investment_funds\": record.get(\"Investment_funds\", \"NULL\"),\n",
    "            \"Exit_summary\": record.get(\"Exit_summary\", \"NULL\"),\n",
    "            \"Total_funding\": record.get(\"Total_funding\", \"NULL\"),\n",
    "            \"Advisors\": record.get(\"Advisors\", \"NULL\"),\n",
    "            \"LinkedIn_URL\": record.get(\"LinkedIn_URL\", \"NULL\"),\n",
    "            \"IPO_summary\": record.get(\"IPO_summary\", \"NULL\"),\n",
    "            \"Value_of_the_startup\": record.get(\"Value_of_the_startup\", \"NULL\"),\n",
    "            \"Number_of_patents\": record.get(\"Number_of_patents\", \"NULL\"),\n",
    "            \"Number_of_trademarks\": record.get(\"Number_of_trademarks\", \"NULL\"),\n",
    "            \"Operating_status\": record.get(\"Operating_status\", \"NULL\"),\n",
    "            \"Type_of_latest_investment\": record.get(\"Type_of_latest_investment\", \"NULL\"),\n",
    "            \"Acquired_by\": record.get(\"Acquired_by\", \"NULL\"),\n",
    "            \"Video_demo\": record.get(\"Video_demo\", \"NULL\"),\n",
    "            \"Website\": record.get(\"Website\", \"NULL\"),\n",
    "            \"Revenue\": record.get(\"Revenue\", \"NULL\"),\n",
    "            \"Growth_rate\": record.get(\"Growth_rate\", \"NULL\"),\n",
    "            \"Logo_url\": record.get(\"Logo_url\", \"NULL\"),\n",
    "            \"Key\" : record.get(\"Key\", \"NULL\"),\n",
    "            \"google_news_urls\": record.get(\"google_news_urls\", \"NULL\")#,\n",
    "            #\"timestamp\": record.get(\"timestamp\", \"NULL\")\n",
    "                    }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "}\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    logger.info(f\"Response status code: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        if is_json(response):\n",
    "            try:\n",
    "                response_json = response.json()\n",
    "                #logger.info(f\"Record added: {data['Name']}\")\n",
    "                #return response_json\n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(f\"Failed to parse response JSON: {e}\")\n",
    "                logger.error(f\"Response text: {response.text}\")\n",
    "        else:\n",
    "            logger.error(\"Response is not JSON.\")\n",
    "            logger.error(f\"Response text: {response.text}\")\n",
    "    else:\n",
    "        logger.error(f\"Failed to add record: {response.status_code}\")\n",
    "        logger.error(f\"Response text: {response.text}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################\n",
    "######## NEWS RETRIEVAL #########\n",
    "##################################\n",
    "\n",
    "\n",
    "def google_news_search(query, num_results=10):\n",
    "    try:\n",
    "        return list(search(query, num=num_results, stop=num_results, tbs='qdr:y'))  # 'tbs=qdr:y' filters for past year\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Google News search: {e}\")\n",
    "        return []\n",
    "\n",
    "def summarize_news_articles(articles_text):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, api_key=openai.api_key)\n",
    "\n",
    "    # Prompt template per il riassunto\n",
    "    template = '''\n",
    "    Below is the text from multiple articles about a startup. Provide a concise summary of only 3 of the articles, separated by \";;;\":\n",
    "    Choose carefully which ones to summarize without repeating yourself, and prioritize articles that discuss different aspects.\n",
    "    Include the name of the source, the article's publication date, and the author's name (if available; if not, leave it out).\n",
    "    The articles must be written in English and only include summaries of articles that mention and discuss the startup.\n",
    "    If there are fewer than 3 relevant articles, summarize fewer.\n",
    "    {articles_text}\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Creazione del prompt\n",
    "    user_prompt = PromptTemplate(template=template, input_variables=[\"articles_text\"])\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([HumanMessagePromptTemplate(prompt=user_prompt)])\n",
    "    llm_chain = LLMChain(prompt=chat_prompt, llm=llm, verbose=False)\n",
    "\n",
    "    # Chiamata all'API di GPT-4o-mini per ottenere il riassunto\n",
    "    result = llm_chain.run({\"articles_text\": articles_text})\n",
    "    \n",
    "    return result\n",
    "\n",
    "def news(startup_urls):\n",
    "    for startup_url in startup_urls:\n",
    "        print(f\"Processing {startup_url}...\")\n",
    "        \n",
    "        # Step 1: Search Google News for related articles \n",
    "        query = f\"{startup_url} startup news\"\n",
    "        search_results = google_news_search(query)\n",
    "        \n",
    "        if not search_results:\n",
    "            print(f\"No search results found for {startup_url}\")\n",
    "            continue\n",
    "\n",
    "        # Step 2: Scrape text from each found article\n",
    "        concatenated_text = \"\"\n",
    "        for result in search_results:\n",
    "            concatenated_text += get_all_text(result, only_main=True) + \" ---- END ---- \"\n",
    "\n",
    "        # Step 3: Generate summaries for the concatenated articles\n",
    "        summarized_text = summarize_news_articles(concatenated_text)\n",
    "    \n",
    "    return summarized_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "##################################\n",
    "# CREAZIONE DELLA CHIAVE UNIVOCA #\n",
    "##################################\n",
    "\n",
    "# Funzione per preprocessare la stringa\n",
    "def preprocess_string(s):\n",
    "    s = s.lower()  # Converti in minuscolo\n",
    "    s = re.sub(r'[^\\w.]', '', s)  # Remove all non-alphanumeric characters except dot (.)\n",
    "    return s\n",
    "\n",
    "# Funzione per normalizzare un URL\n",
    "def normalize_url(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    # Prendi solo il netloc (dominio) e il percorso (escludi http/https e www)\n",
    "    normalized_url = parsed_url.netloc.replace('www.', '') + parsed_url.path\n",
    "    # Rimuovi eventuali slash finali\n",
    "    normalized_url = normalized_url.rstrip('/')\n",
    "    return normalized_url\n",
    "\n",
    "# Funzione per generare una chiave univoca utilizzando solo il website\n",
    "def generate_unique_key(website):\n",
    "    website = preprocess_string(normalize_url(website))\n",
    "    unique_key = hashlib.sha256(website.encode()).hexdigest()\n",
    "    return unique_key\n",
    "\n",
    "\n",
    "#############################\n",
    "# ESTRAZIONE LINKS INTERESSANTI\n",
    "#############################\n",
    "def analyze_links_with_gpt(links):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, api_key=openai.api_key)\n",
    "\n",
    "    # Template per il prompt di sistema\n",
    "    system_template = (\n",
    "        \"Sei un assistente esperto che aiuta a identificare link web che contengono liste o ranking di startup. \"\n",
    "        \"Rispondi con una lista di link che parlano di liste o ranking di startup, uno per riga, senza numerazione o testo aggiuntivo.\"\n",
    "    )\n",
    "\n",
    "    # Template per il prompt dell'utente\n",
    "    template = '''\n",
    "    Ecco una lista di link web. Per favore, identifica quali di questi contengono liste o ranking di startup e restituisci solo i link:\n",
    "    {links}\n",
    "    '''\n",
    "\n",
    "    # Creazione del prompt\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    user_prompt = PromptTemplate(template=template, input_variables=[\"links\"])\n",
    "    user_message_prompt = HumanMessagePromptTemplate(prompt=user_prompt)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, user_message_prompt])\n",
    "    llm_chain = LLMChain(prompt=chat_prompt, llm=llm, verbose=False)\n",
    "\n",
    "    # Prepara i link come una stringa unica separata da nuovi linee\n",
    "    links_str = \"\\n\".join(links)\n",
    "\n",
    "    # Chiamata all'API di GPT-4 per ottenere il risultato filtrato\n",
    "    result = llm_chain.run({\"links\": links_str})\n",
    "    \n",
    "    # Post-processing: Rimuovi eventuali spazi vuoti e filtra solo le righe che contengono link validi\n",
    "    filtered_links = [link.strip() for link in result.split(\"\\n\") if link.strip().startswith(\"http\")]\n",
    "    \n",
    "    return filtered_links\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "# CREAZIONE DIZIONARIO STARTUP\n",
    "###############################\n",
    "\n",
    "dict_example = [\n",
    "                {'name': 'startup 1',\n",
    "                 'website': 'website 1',\n",
    "                 'venture_radar_profile': 'profile 1'},\n",
    "                {'name': 'startup 2',\n",
    "                 'website': 'website 2',\n",
    "                 'venture_radar_profile': 'profile 2'}\n",
    "                 ]\n",
    "\n",
    "# Funzione per analizzare il contenuto della pagina della startup\n",
    "def analyze_startup_page_with_gpt(html_content, dict_example):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, api_key=openai.api_key)\n",
    "\n",
    "    # Template per il prompt di sistema\n",
    "    system_template = (\n",
    "        \"Sei un assistente esperto che analizza il contenuto di una pagina web per estrarre le informazioni principali \"\n",
    "        \"relative a una startup, come il nome della startup, l'URL del sito della startup e l'URL del profilo della startup su Venture Radar. \"\n",
    "        \"Restituisci queste informazioni come lista python di dizionari, senza la formattazione markdown: rispondi solamente con quello richiesto.\"\n",
    "        \"il seguente Ã¨ un esempio di output: {dict_example}\"\n",
    "        )\n",
    "\n",
    "    # Template per il prompt dell'utente\n",
    "    template = '''\n",
    "    Qui di seguito c'Ã¨ il contenuto HTML di una pagina web che descrive una o piÃ¹ startup. \n",
    "    Estrai e restituisci le seguenti informazioni per ogni startup trovata, nel formato richiesto:\n",
    "    - Nome della startup\n",
    "    - URL del sito della startup\n",
    "    - URL del profilo della startup su Venture Radar\n",
    "    \n",
    "    Contenuto HTML della pagina:\n",
    "    {html_content}\n",
    "    '''\n",
    "\n",
    "    # Creazione del prompt\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "    user_prompt = PromptTemplate(template=template, input_variables=[\"html_content\", \"dict_example\"])\n",
    "    user_message_prompt = HumanMessagePromptTemplate(prompt=user_prompt)\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, user_message_prompt])\n",
    "    llm_chain = LLMChain(prompt=chat_prompt, llm=llm, verbose=False)\n",
    "\n",
    "    # Chiamata all'API di GPT-4 per ottenere il risultato analizzato\n",
    "    result = llm_chain.run({\"html_content\": html_content, \"dict_example\": dict_example})\n",
    "    \n",
    "    try:\n",
    "        # Tenta di caricare il risultato come JSON\n",
    "        startup_data = eval(result)\n",
    "        return startup_data\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to eval generated dict, returning raw result\")\n",
    "        print(\"Raw result:\", result)\n",
    "        return result  # Ritorna il risultato grezzo per ulteriori analisi\n",
    "\n",
    "\n",
    "##############################\n",
    "## LINK PROCESSING VENTURE\n",
    "##############################\n",
    "\n",
    "# Funzione per ottenere il link al sito web dalla pagina Venture Radar\n",
    "def get_website_from_ventureradar(link):\n",
    "    # Usa la funzione per inizializzare o resettare il driver\n",
    "    driver = reset_driver()\n",
    "    driver.get(link)\n",
    "    time.sleep(2)  # Attendi il caricamento della pagina\n",
    "    pageContent = driver.page_source\n",
    "    soup = BeautifulSoup(pageContent, 'html.parser')\n",
    "    website_div = soup.find('div', id='i_d_CompanyWebsiteLink')\n",
    "    if website_div and website_div.find('a', href=True):\n",
    "        return website_div.find('a')['href']\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def is_valid_url(url, expected_domain=None):\n",
    "    try:\n",
    "        result = re.match(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', url)\n",
    "        if expected_domain:\n",
    "            return result and expected_domain in url\n",
    "        return result is not None\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def process_startups(startup_data):\n",
    "    processed_startups = {}\n",
    "\n",
    "    # Itera su ogni startup nel dizionario\n",
    "    for startup in startup_data.get('page_1', []):\n",
    "        startup_name = startup.get('name')\n",
    "        startup_url = startup.get('website')\n",
    "        venture_radar_url = startup.get('venture_radar_profile')\n",
    "\n",
    "        # Caso 1: Sia il sito web che il sito interno sono valorizzati\n",
    "        if startup_url and venture_radar_url:\n",
    "            if not is_valid_url(venture_radar_url, \"ventureradar.com\") or startup_url == venture_radar_url:\n",
    "                # Se il sito interno non Ã¨ valido o Ã¨ uguale al sito web, ricostruiamo il sito interno\n",
    "                venture_radar_url = f\"https://www.ventureradar.com/organisation/{startup_name.replace(' ', '%20')}\"\n",
    "            processed_startups[startup_name] = {\n",
    "                \"Startup URL\": startup_url,\n",
    "                \"Venture Radar URL\": venture_radar_url\n",
    "            }\n",
    "\n",
    "        # Caso 2: Solo il sito interno Ã¨ valorizzato\n",
    "        elif venture_radar_url:\n",
    "            if is_valid_url(venture_radar_url, \"ventureradar.com\"):\n",
    "                # Recuperiamo il sito web dalla pagina Venture Radar\n",
    "                startup_url = get_website_from_ventureradar(venture_radar_url)\n",
    "                if startup_url:\n",
    "                    processed_startups[startup_name] = {\n",
    "                        \"Startup URL\": startup_url,\n",
    "                        \"Venture Radar URL\": venture_radar_url\n",
    "                    }\n",
    "\n",
    "        # Caso 3: Solo il sito web Ã¨ valorizzato\n",
    "        elif startup_url:\n",
    "            # Costruiamo il sito interno\n",
    "            venture_radar_url = f\"https://www.ventureradar.com/organisation/{startup_name.replace(' ', '%20')}\"\n",
    "            processed_startups[startup_name] = {\n",
    "                \"Startup URL\": startup_url,\n",
    "                \"Venture Radar URL\": venture_radar_url\n",
    "            }\n",
    "\n",
    "        # Caso 4: Nessuno dei due URL Ã¨ valorizzato\n",
    "        else:\n",
    "            pass\n",
    "            # Non facciamo nulla, la startup non viene inclusa\n",
    "\n",
    "    return processed_startups\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "### CAPTCHA RESOLVING\n",
    "##############################\n",
    "\n",
    "def human_like_scroll(driver, pause_time=2):\n",
    "    \"\"\"Simula lo scorrimento umano con pause casuali.\"\"\"\n",
    "    total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    for i in range(1, total_height, random.randint(300, 600)):\n",
    "        driver.execute_script(f\"window.scrollTo(0, {i});\")\n",
    "        time.sleep(random.uniform(pause_time - 1, pause_time + 1))\n",
    "\n",
    "def safe_mouse_movement(driver):\n",
    "    \"\"\"Muove il mouse su un elemento sicuro per evitare errori di movimento.\"\"\"\n",
    "    actions = ActionChains(driver)\n",
    "    try:\n",
    "        element = driver.find_element(By.TAG_NAME, 'body')\n",
    "        actions.move_to_element(element).perform()\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "    except MoveTargetOutOfBoundsException:\n",
    "        print(\"Move target out of bounds, skipping mouse movement.\")\n",
    "\n",
    "def random_click(driver):\n",
    "    \"\"\"Simula un click casuale su un elemento interattivo.\"\"\"\n",
    "    elements = driver.find_elements(By.XPATH, \"//a | //button\")\n",
    "    interactive_elements = []\n",
    "    \n",
    "    for element in elements:\n",
    "        try:\n",
    "            if element.is_displayed() and element.is_enabled():\n",
    "                interactive_elements.append(element)\n",
    "        except (ElementNotInteractableException, StaleElementReferenceException):\n",
    "            continue\n",
    "\n",
    "    if interactive_elements:\n",
    "        random.choice(interactive_elements).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROWJO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WebDriver\n",
    "base_url2 = \"https://growjo.com/\"\n",
    "\n",
    "\n",
    "# Loop through pages from 2 to 195\n",
    "for idxx, page_num in enumerate(range(1, 2)):\n",
    "    page_url = f\"https://growjo.com/home/{page_num}\"\n",
    "    print('Scraped page:', idxx+1)\n",
    "\n",
    "    try:\n",
    "        # Usa la funzione per inizializzare o resettare il driver\n",
    "        driver = reset_driver()\n",
    "        # Fetch the page\n",
    "        driver.get(page_url)\n",
    "        pageContent = driver.page_source\n",
    "        soup = BeautifulSoup(pageContent, 'html.parser')\n",
    "\n",
    "        # Find the main exhibitors div\n",
    "        startup_containers = soup.find_all('tr', class_='jss33 jss35', role='checkbox', tabindex='-1')\n",
    "\n",
    "        # For each row in the table\n",
    "        for idxxx, row in enumerate(startup_containers):\n",
    "            print('scraping of startup number', 50 * (idxx) + (idxxx + 1) )\n",
    "\n",
    "            data = []\n",
    "            try:\n",
    "                # Extract ranking\n",
    "                ranking = row.find('div', class_='ranking-wrapper').text.strip() if row.find('div', class_='ranking-wrapper') else ''\n",
    "                \n",
    "                # Extract company name and link\n",
    "                company_cell = row.find_all('td', class_='text-start')[0]\n",
    "                company_name = company_cell.find('a', href=lambda x: x and x.startswith('/company/')).text.strip() if company_cell.find('a') else ''\n",
    "                print(company_name)\n",
    "                company_link = company_cell.find('a', href=lambda x: x and x.startswith('/company/'))['href'] \n",
    "                \n",
    "                # Extract location details\n",
    "                location_cell = row.find_all('td', class_='text-start')[1]\n",
    "                location = location_cell.find('a', class_='custom-anchor')['href'].split('/')[-1] if location_cell.find('a') else ''\n",
    "                \n",
    "                # Extract country\n",
    "                country = row.find_all('td', class_='text-start')[2].text.strip() if len(row.find_all('td', class_='text-start')) > 2 else ''\n",
    "                \n",
    "                # Extract funding details\n",
    "                funding = row.find_all('td', class_='text-start')[3].text.strip() if len(row.find_all('td', class_='text-start')) > 3 else ''\n",
    "                \n",
    "                # Extract industry\n",
    "                industry_cell = row.find_all('td', class_='text-start')[4]\n",
    "                industry = industry_cell.find('a').text.strip() if industry_cell.find('a') else ''\n",
    "                \n",
    "                # Extract employee count\n",
    "                employee_count = row.find_all('td', class_='text-start')[5].text.strip() if len(row.find_all('td', class_='text-start')) > 5 else ''\n",
    "                \n",
    "                # Extract revenue\n",
    "                revenue = row.find_all('td', class_='text-start')[6].text.strip() if len(row.find_all('td', class_='text-start')) > 6 else ''\n",
    "                \n",
    "                # Extract growth rate\n",
    "                growth_rate = row.find_all('td', class_='text-start')[7].text.strip() if len(row.find_all('td', class_='text-start')) > 7 else ''\n",
    "                \n",
    "                # Extract key person name and link\n",
    "                key_person_cell = row.find_all('td', class_='text-start')[8]\n",
    "                key_person = key_person_cell.find('a').text.strip() if key_person_cell.find('a') else ''\n",
    "                key_person_link = key_person_cell.find('a')['href'] if key_person_cell.find('a') else ''\n",
    "                \n",
    "                # Extract position\n",
    "                position = row.find_all('td', class_='text-start')[9].text.strip() if len(row.find_all('td', class_='text-start')) > 9 else ''\n",
    "                \n",
    "                # single startup page\n",
    "                url_startup = base_url2[:-1] + company_link\n",
    "                # Usa la funzione per inizializzare o resettare il driver\n",
    "                driver = reset_driver()\n",
    "                driver.get(url_startup)\n",
    "\n",
    "                # prendo tutto il testo nella pagina della startup\n",
    "                main_text = get_all_text(url_startup, only_main = True)\n",
    "\n",
    "                # Wait for the exhibitors list to be present\n",
    "                pageContent = driver.page_source\n",
    "\n",
    "                # Parse the main page with BeautifulSoup\n",
    "                soup = BeautifulSoup(pageContent, 'html.parser')\n",
    "\n",
    "                # Extract body\n",
    "                startup_body = soup.find('div', class_='tempCompany card-body')\n",
    "\n",
    "                # Logo extraction\n",
    "                rectangle = startup_body.find('div', id = 'revenue-financials')\n",
    "                logo_element = rectangle.find('img', src=lambda x: x and \"logo.clearbit.com\" in x)\n",
    "                logo_url = 'https:' + logo_element['src'] if logo_element else None\n",
    "                \n",
    "                # Extract company link\n",
    "                company_link_element = rectangle.find('a', href=True)\n",
    "                company_link = 'https:' + company_link_element['href'] if company_link_element else None\n",
    "                \n",
    "                # Employee extraction\n",
    "                employee_internal_links = [base_url2[:-1] + a['href'] for a in soup.find_all('a', href=lambda x: x and \"/employee/\" in x)]\n",
    "                employee_internal_links = ', '.join(employee_internal_links)\n",
    "\n",
    "                # Append data to the list\n",
    "                data.append([\n",
    "                            company_name, country, location, funding,\n",
    "                            industry, employee_count, revenue, growth_rate, key_person,\n",
    "                            key_person_link, position, logo_url, company_link\n",
    "                        ])\n",
    "\n",
    "                new_data = pd.DataFrame(data, columns=[\"Startup Name\", \"Country\", \"City\", \"Total funding\", \"Industry\", \n",
    "                                       \"Number of employees\", \"Revenue\", \"Growth Rate\", \"Key Person\", \n",
    "                                       \"Key Person Link\", \"Position\", \"Logo URL\", \"Website URL\"])\n",
    "\n",
    "                # GPT-4 WORK\n",
    "                initial_data = new_data.to_dict()\n",
    "                url = new_data['Website URL'][0]\n",
    "\n",
    "                text2 = get_all_text(url)\n",
    "                text2 += main_text\n",
    "                text_news = news([company_name])\n",
    "                text2 += text_news\n",
    "                print('get_all_text Ã¨ andato OK')\n",
    "\n",
    "                analysis_result = analyze_text_with_gpt(text2, initial_data)\n",
    "                print('analyze_text_with_gpt Ã¨ andato OK')\n",
    "\n",
    "                # Convert the analysis result into a dictionary\n",
    "                data_dict = convert_to_dict(analysis_result)\n",
    "                print('convert_to_dict Ã¨ andato OK')\n",
    "\n",
    "                data_dict['google_news_urls'] = text_news\n",
    "\n",
    "                data_dict['Key'] = generate_unique_key(data_dict['Website'])\n",
    "\n",
    "                data_dict['timestamp'] = datetime.now().isoformat()\n",
    "                \n",
    "                # Convert the dictionary into a DataFrame\n",
    "                df = pd.DataFrame([data_dict])\n",
    "\n",
    "                # Upload the first record in 'df' to Airtable\n",
    "                upload_record_to_airtable(df.iloc[0].to_dict(), url_airtable, headers)\n",
    "                print('upload_record_to_airtable Ã¨ andato OK')\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to scrape row: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape page {page_num}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VENTURE RADAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WebDriver\n",
    "driver = reset_driver()\n",
    "\n",
    "# URL di base\n",
    "base_url = \"https://blog.ventureradar.com/\"\n",
    "\n",
    "# Apri la pagina con Selenium\n",
    "driver.get(base_url)\n",
    "\n",
    "# Simula lo scroll fino in fondo alla pagina per caricare nuovi articoli\n",
    "SCROLL_PAUSE_TIME = 2\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scorri verso il basso\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Aspetta il caricamento della pagina\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Calcola l'altezza nuova della pagina dopo lo scroll\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # Esci dal ciclo se non ci sono nuovi contenuti da caricare\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Ottieni il contenuto completo della pagina dopo aver caricato tutti gli articoli\n",
    "pageContent = driver.page_source\n",
    "\n",
    "# Usa BeautifulSoup per analizzare il contenuto della pagina\n",
    "soup = BeautifulSoup(pageContent, 'html.parser')\n",
    "\n",
    "# Trova tutti gli oggetti di tipo <article>\n",
    "articles = soup.find_all('article')\n",
    "\n",
    "# Lista per memorizzare i link trovati\n",
    "links = []\n",
    "\n",
    "# Itera su ogni <article> per estrarre il link\n",
    "for article in articles:\n",
    "    link = article.find('a', href=True)\n",
    "    if link:\n",
    "        links.append(link['href'])\n",
    "\n",
    "filtered_links = analyze_links_with_gpt(links)\n",
    "\n",
    "\n",
    "# Dizionario per memorizzare le informazioni di tutte le startup\n",
    "startups_info = {}\n",
    "\n",
    "# Itera su ogni link filtrato\n",
    "for idx, link in enumerate(filtered_links):\n",
    "    print(f\"Processing link number: {idx+1} of {len(filtered_links)} total links\")\n",
    "    \n",
    "    # Apri la pagina del link con Selenium\n",
    "    # Usa la funzione per inizializzare o resettare il driver\n",
    "    driver = reset_driver()\n",
    "    driver.get(link)\n",
    "    time.sleep(3)  # Attendi il caricamento della pagina\n",
    "    \n",
    "    # Ottieni il contenuto della pagina\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "    # Estrai il contenuto del div con la classe \"entry-content\"\n",
    "    entry_content_div = soup.find('div', class_='entry-content')\n",
    "    \n",
    "    if entry_content_div:\n",
    "        # Converti il contenuto del div in una stringa HTML\n",
    "        html_content = str(entry_content_div)\n",
    "        \n",
    "        # Analizza il contenuto HTML della pagina per estrarre le informazioni delle startup\n",
    "        startup_data = analyze_startup_page_with_gpt(html_content, dict_example)\n",
    "        \n",
    "        # Aggiungi le informazioni al dizionario principale\n",
    "        startups_info[f\"page_{idx+1}\"] = startup_data\n",
    "    else:\n",
    "        print(f\"No entry-content found for link: {link}\")\n",
    "\n",
    "    break \n",
    "\n",
    "# Elaborazione dei dati delle startup\n",
    "processed_startups_info = process_startups(startups_info)\n",
    "\n",
    "\n",
    "# Itero sul dizionario cosÃ¬ composto:\n",
    "# {\n",
    "# nome startup : {url website, url sito interno vendure},\n",
    "# ...\n",
    "# }\n",
    "\n",
    "for startup_name, urls in processed_startups_info.items():\n",
    "        print(f\"Processing startup: {startup_name}\")\n",
    "        \n",
    "        # Ottieni il testo completo dal sito web della startup\n",
    "        startup_url = urls.get('Startup URL')\n",
    "        if startup_url:\n",
    "            startup_text = get_all_text(startup_url)\n",
    "        else:\n",
    "            startup_text = \"\"\n",
    "        \n",
    "        internal_startup_url = urls.get('Venture Radar URL')\n",
    "\n",
    "        if internal_startup_url:\n",
    "            #internal_startup_text = get_all_text(internal_startup_url, only_main=True)\n",
    "            # Usa la funzione per inizializzare o resettare il driver\n",
    "            driver = reset_driver()\n",
    "            driver.get(internal_startup_url)\n",
    "            time.sleep(3)  # Attendi il caricamento della pagina\n",
    "    \n",
    "            # Ottieni il contenuto della pagina\n",
    "            page_content = driver.page_source\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "            # Estrai il contenuto del div con la classe \"entry-content\"\n",
    "            internal_startup_text = soup.find('div', class_='container c_d_ProfileContainer')\n",
    "\n",
    "        else:\n",
    "            internal_startup_text = \"\"\n",
    "\n",
    "        # Ottieni le notizie relative alla startup\n",
    "        news_summary = news([startup_name])  \n",
    "\n",
    "        # Concatena le informazioni per l'analisi GPT\n",
    "        full_text = f\"Startup Information:\\n{startup_text}\\n\\nNews Summary:\\n{news_summary}\"\n",
    "        \n",
    "        # Analizza il testo concatenato usando GPT\n",
    "        gpt_analysis = analyze_text_with_gpt(full_text, initial_data = {internal_startup_text})\n",
    "        data_dict = convert_to_dict(gpt_analysis)\n",
    "        \n",
    "        data_dict['google_news_urls'] = news_summary\n",
    "        data_dict['Key'] = generate_unique_key(startup_url)\n",
    "\n",
    "        # Convert the dictionary into a DataFrame\n",
    "        df = pd.DataFrame([data_dict])\n",
    "\n",
    "        # Upload the first record in 'df' to Airtable\n",
    "        upload_record_to_airtable(df.iloc[0].to_dict(), url_airtable, headers)\n",
    "        print('upload_record_to_airtable Ã¨ andato OK')\n",
    "        \n",
    "        break            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STARTUP RANKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WebDriver\n",
    "driver = reset_driver()\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://www.startupranking.com/startup/new\"\n",
    "\n",
    "# Open the webpage\n",
    "driver.get(url)\n",
    "\n",
    "# Allow some time for the page to load completely\n",
    "time.sleep(3)\n",
    "\n",
    "# Get the page source\n",
    "page_content = driver.page_source\n",
    "\n",
    "# Parse the page content with BeautifulSoup\n",
    "soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "# Find the table element by class name\n",
    "table = soup.find('table', class_='table-striped wide new-startups')\n",
    "\n",
    "# Check if the table was found\n",
    "if table:\n",
    "    # Prepare a list to hold the data\n",
    "    data = []\n",
    "\n",
    "    # Extract rows from the table\n",
    "    rows = table.find_all('tr')\n",
    "    for idx, row in enumerate(rows[1:]):\n",
    "        # Extract the <div> with class \"name\" from each row\n",
    "        name_div = row.find('div', class_='name')\n",
    "        if name_div:\n",
    "            # Get the text content for the startup name\n",
    "            name = name_div.get_text(strip=True)\n",
    "            \n",
    "            # Get the link to the startup's profile\n",
    "            link = name_div.find('a', href=True)['href']\n",
    "            full_link = f\"https://www.startupranking.com{link}\"\n",
    "            \n",
    "            # Visit the startup's profile page\n",
    "            driver.get(full_link)\n",
    "            time.sleep(2)  # Allow time for the page to load\n",
    "\n",
    "            # Get the page content\n",
    "            profile_page_content = driver.page_source\n",
    "            profile_soup = BeautifulSoup(profile_page_content, 'html.parser')\n",
    "            \n",
    "            # Extract the content from the div with class 'wrapper container group'\n",
    "            wrapper_div = profile_soup.find('div', class_='wrapper container group')\n",
    "            if wrapper_div:\n",
    "                html_content = str(wrapper_div)\n",
    "\n",
    "            # estraggo il sito web della startup e tutto il testo\n",
    "            website = wrapper_div.find('p', class_ = \"su-loc\").find('a', href=True).text\n",
    "            if website:\n",
    "                startup_text = get_all_text(website)\n",
    "            else:\n",
    "                startup_text = \"\"\n",
    "\n",
    "            # Ottieni le notizie relative alla startup\n",
    "            news_summary = news([name]) \n",
    "            \n",
    "            # Concatena le informazioni per l'analisi GPT\n",
    "            full_text = f\"Startup Information:\\n{startup_text}\\n\\nNews Summary:\\n{news_summary}\\n\\nHTML of Startup profile:\\n{html_content}\"\n",
    "\n",
    "            # Analyze the content with GPT\n",
    "            analysis_result = analyze_text_with_gpt(full_text, [])\n",
    "            data_dict = convert_to_dict(analysis_result)\n",
    "        \n",
    "            data_dict['google_news_urls'] = news_summary\n",
    "            data_dict['Key'] = generate_unique_key(website)\n",
    "\n",
    "            # Convert the dictionary into a DataFrame\n",
    "            df = pd.DataFrame([data_dict])\n",
    "\n",
    "            # Upload the first record in 'df' to Airtable\n",
    "            upload_record_to_airtable(df.iloc[0].to_dict(), url_airtable, headers)\n",
    "            print('upload_record_to_airtable Ã¨ andato OK')\n",
    "                    \n",
    "            break\n",
    "\n",
    "else:\n",
    "    print(\"Table not found!\")\n",
    "\n",
    "# Quit the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STARTUPEABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WebDriver\n",
    "driver = reset_driver()\n",
    "\n",
    "# Prepare a list to hold the data\n",
    "data = []\n",
    "\n",
    "# Iterate over the pages\n",
    "for page in range(1, 97):  # Pages range from 1 to 96\n",
    "    # URL to scrape\n",
    "    url = f\"https://startupeable.com/directorio/explora/?pg={page}&sort=a-z\"\n",
    "\n",
    "    # Open the webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Allow some time for the page to load completely\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Get the page source\n",
    "    page_content = driver.page_source\n",
    "\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "    # Find the main div containing the \"table\"\n",
    "    results_view = soup.find('div', class_='row results-view grid fc-type-2-results')\n",
    "\n",
    "    # Check if the div was found\n",
    "    if results_view:\n",
    "        # Extract each startup block within the results-view div\n",
    "        startups = results_view.find_all('div', class_='grid-item')\n",
    "\n",
    "        for startup in startups:\n",
    "            # Extract the name from the h4 tag\n",
    "            name_tag = startup.find('h4', class_='case27-secondary-text listing-preview-title')\n",
    "            name = name_tag.get_text(strip=True) if name_tag else None\n",
    "\n",
    "            # Extract the link to the startup's profile\n",
    "            link_tag = startup.find('a', href=True)\n",
    "            profile_link = link_tag['href'] if link_tag else None\n",
    "\n",
    "            # Add the name and link to the data list\n",
    "            if name and profile_link:\n",
    "                data.append({'Name': name, 'Profile Link': profile_link})\n",
    "            \n",
    "\n",
    "            # Visit the startup's profile page\n",
    "            driver = reset_driver()\n",
    "            driver.get(profile_link)\n",
    "            time.sleep(2)  # Allow time for the page to load\n",
    "\n",
    "            # Get the page content\n",
    "            profile_page_content = driver.page_source\n",
    "            profile_soup = BeautifulSoup(profile_page_content, 'html.parser')\n",
    "            \n",
    "            # Extract the content from the div with class 'wrapper container group'\n",
    "            wrapper_div = profile_soup.find('div', class_='single-job-listing')\n",
    "            if wrapper_div:\n",
    "                html_content = str(wrapper_div)\n",
    "            else:\n",
    "                html_content = \"\"\n",
    "\n",
    "            # estraggo il sito web della startup e tutto il testo\n",
    "            website = wrapper_div.find('a', rel=\"nofollow\", target=\"_blank\")['href']\n",
    "            if website:                \n",
    "                startup_text = get_all_text(website)\n",
    "            else:\n",
    "                startup_text = \"\"\n",
    "\n",
    "                       # Ottieni le notizie relative alla startup\n",
    "            news_summary = news([name]) \n",
    "            \n",
    "            # Concatena le informazioni per l'analisi GPT\n",
    "            full_text = f\"Startup Information:\\n{startup_text}\\n\\nNews Summary:\\n{news_summary}\\n\\nHTML of Startup profile:\\n{html_content}\"\n",
    "\n",
    "            # Analyze the content with GPT\n",
    "            analysis_result = analyze_text_with_gpt(full_text, [])\n",
    "            data_dict = convert_to_dict(analysis_result)\n",
    "        \n",
    "            data_dict['google_news_urls'] = news_summary\n",
    "            data_dict['Key'] = generate_unique_key(website)\n",
    "\n",
    "            # Convert the dictionary into a DataFrame\n",
    "            df = pd.DataFrame([data_dict])\n",
    "\n",
    "            # Upload the first record in 'df' to Airtable\n",
    "            upload_record_to_airtable(df.iloc[0].to_dict(), url_airtable, headers)\n",
    "            print('upload_record_to_airtable Ã¨ andato OK')\n",
    "        \n",
    "\n",
    "\n",
    "            break \n",
    "     \n",
    "    else:\n",
    "        print(f\"No results-view found on page {page}!\")\n",
    "\n",
    "    break\n",
    "\n",
    "# Quit the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE HUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WebDriver\n",
    "driver = reset_driver()\n",
    "\n",
    "# Prepare a list to hold the data\n",
    "data = []\n",
    "\n",
    "# Iterate over the pages\n",
    "for page in range(1, 20):  # Pages range from 1 to 96\n",
    "    # URL to scrape\n",
    "    url = f\"https://thehub.io/startups?industries=retail&page={page}\"\n",
    "\n",
    "    # Open the webpage\n",
    "    driver.get(url)\n",
    "\n",
    "    # Allow some time for the page to load completely\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Get the page source\n",
    "    page_content = driver.page_source\n",
    "\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "    # Find the main div containing the \"table\"\n",
    "    results_view = soup.find_all('div', class_='mb-30 col-md-6 col-lg-4')\n",
    "\n",
    "    # loop sui risultati trovati nella pagina\n",
    "    for el in results_view:\n",
    "        internal_link = \"https://thehub.io\" + el.find('a')['href']\n",
    "        name = el.find('h4', class_ = 'card-title').text\n",
    "\n",
    "        # Visit the startup's profile page\n",
    "        driver = reset_driver()\n",
    "        driver.get(internal_link)\n",
    "        time.sleep(2)  # Allow time for the page to load\n",
    "\n",
    "        # Get the page content\n",
    "        profile_page_content = driver.page_source\n",
    "        profile_soup = BeautifulSoup(profile_page_content, 'html.parser')\n",
    "        \n",
    "        # prendo html della pagina interna\n",
    "        wrapper_div = profile_soup.find('div', class_ = 'company-form-preview')\n",
    "        if wrapper_div:\n",
    "            html_content = str(wrapper_div)\n",
    "        else:\n",
    "            html_content = \"\"\n",
    "\n",
    "        # estraggo il sito web della startup e tutto il testo\n",
    "        website = wrapper_div.find('a', class_ = 'text-blue-900')['href']\n",
    "        if website:                \n",
    "            startup_text = get_all_text(website)\n",
    "        else:\n",
    "            startup_text = \"\"\n",
    "\n",
    "        # Ottieni le notizie relative alla startup\n",
    "        news_summary = news([name]) \n",
    "            \n",
    "        # Concatena le informazioni per l'analisi GPT\n",
    "        full_text = f\"Startup Information:\\n{startup_text}\\n\\nNews Summary:\\n{news_summary}\\n\\nHTML of Startup profile:\\n{html_content}\"\n",
    "\n",
    "        # Analyze the content with GPT\n",
    "        analysis_result = analyze_text_with_gpt(full_text, [])\n",
    "        data_dict = convert_to_dict(analysis_result)\n",
    "        \n",
    "        data_dict['google_news_urls'] = news_summary\n",
    "        data_dict['Key'] = generate_unique_key(website)\n",
    "\n",
    "        # Convert the dictionary into a DataFrame\n",
    "        df = pd.DataFrame([data_dict])\n",
    "\n",
    "        # Upload the first record in 'df' to Airtable\n",
    "        upload_record_to_airtable(df.iloc[0].to_dict(), url_airtable, headers)\n",
    "        print('upload_record_to_airtable Ã¨ andato OK')    \n",
    "\n",
    "        \n",
    "        break\n",
    "\n",
    "        \n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WELLFOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WebDriver\n",
    "for page_number in range(1, 2801):\n",
    "    driver = initialize_driver(headless=True)\n",
    "    # Vai alla pagina web\n",
    "    driver.get(f\"https://wellfound.com/startups?page={page_number}\")\n",
    "\n",
    "    # Simula comportamenti umani\n",
    "    time.sleep(random.uniform(5, 10))  # Pausa iniziale casuale\n",
    "    human_like_scroll(driver, pause_time=random.uniform(2, 5))  # Scorrimento umano\n",
    "    print('human_like_scroll OK')\n",
    "    safe_mouse_movement(driver)  # Movimento del mouse su un elemento sicuro\n",
    "    print('safe_mouse_movement OK')\n",
    "    random_click(driver)  # Click su un elemento interattivo\n",
    "    print('random_click OK')\n",
    "\n",
    "    # Attendi che la pagina si carichi completamente\n",
    "    time.sleep(random.uniform(5, 10))\n",
    "\n",
    "    # Estrai il contenuto della pagina\n",
    "    page_content = driver.page_source\n",
    "\n",
    "    # Chiudi il browser\n",
    "    driver.quit()\n",
    "\n",
    "    # Ora puoi usare BeautifulSoup su `page_content`\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "    # Aggiungi una pausa casuale tra le iterazioni per ridurre il rischio di essere bloccato\n",
    "    time.sleep(random.uniform(10, 20))\n",
    "\n",
    "    container = soup.find(\"div\", class_ = \"mt-4 flex w-full flex-col gap-6\")\n",
    "    list_ = container.find_all(\"div\", class_ = \"rounded-lg border border-gray-400 p-8 pb-0\")\n",
    "    for el in list_ : \n",
    "        internal_link = \"https://wellfound.com/\" + el.find('a', class_ = \"styles_component__UCLp3 styles_defaultLink__eZMqw !no-underline\")[\"href\"]\n",
    "        name = el.find(\"header\", class_ = \"text-dark-aaaa font-medium antialiased text-lg\").find_all('div')[0].text\n",
    "        \n",
    "        # Visit the startup's profile page overview\n",
    "        driver = initialize_driver(headless=True)\n",
    "        driver.get(internal_link)\n",
    "        time.sleep(random.uniform(5, 15))  # Allow time for the page to load\n",
    "        profile_page_content = driver.page_source\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "        profile_soup = BeautifulSoup(profile_page_content, 'html.parser')\n",
    "        wrapper_div_overview = profile_soup.find('div', class_ = 'styles_box___OsDD')\n",
    "        if wrapper_div_overview:\n",
    "            html_content_overview = str(wrapper_div_overview)\n",
    "        else:\n",
    "            html_content_overview = \"\" \n",
    "\n",
    "        \n",
    "        # Visit the startup's profile page funding\n",
    "        internal_link_funding = internal_link + \"/funding\"\n",
    "\n",
    "        driver = initialize_driver(headless=True)\n",
    "        driver.get(internal_link_funding)\n",
    "        time.sleep(random.uniform(5, 15))  # Allow time for the page to load\n",
    "        profile_page_content = driver.page_source\n",
    "        driver.quit()\n",
    "\n",
    "        profile_soup = BeautifulSoup(profile_page_content, 'html.parser')\n",
    "        wrapper_div_funding = profile_soup.find('div', class_ = \"styles_component___DsQw\")\n",
    "        if wrapper_div_funding:\n",
    "            html_content_funding = str(wrapper_div_funding)\n",
    "        else:\n",
    "            html_content_funding = \"\" \n",
    "\n",
    "        html_content = html_content_overview + html_content_funding\n",
    "\n",
    "        # estraggo il sito web della startup e tutto il testo\n",
    "        website = wrapper_div_overview.find('button', class_ = \"styles_websiteLink___Rnfc\").text\n",
    "        if website:                \n",
    "            startup_text = get_all_text(website)\n",
    "        else:\n",
    "            startup_text = \"\"\n",
    "\n",
    "        # Ottieni le notizie relative alla startup\n",
    "        news_summary = news([name]) \n",
    "                \n",
    "        # Concatena le informazioni per l'analisi GPT\n",
    "        full_text = f\"Startup Information:\\n{startup_text}\\n\\nNews Summary:\\n{news_summary}\\n\\nHTML of Startup profile:\\n{html_content}\"\n",
    "\n",
    "        # Analyze the content with GPT\n",
    "        analysis_result = analyze_text_with_gpt(full_text, [])\n",
    "        data_dict = convert_to_dict(analysis_result)\n",
    "            \n",
    "        data_dict['google_news_urls'] = news_summary\n",
    "        data_dict['Key'] = generate_unique_key(website)\n",
    "\n",
    "        # Convert the dictionary into a DataFrame\n",
    "        df = pd.DataFrame([data_dict])\n",
    "\n",
    "        # Upload the first record in 'df' to Airtable\n",
    "        upload_record_to_airtable(df.iloc[0].to_dict(), url_airtable, headers)\n",
    "        print('upload_record_to_airtable Ã¨ andato OK')\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI STARTUPS EUROPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10AI GmbH...\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the pages\n",
    "for page in range(1, 125):  # Pages range from 1 to 96\n",
    "    # URL to scrape\n",
    "    url = f\"https://www.ai-startups-europe.eu//p{page}\"\n",
    "\n",
    "    # Open the webpage\n",
    "    # Initialize the WebDriver\n",
    "    driver = reset_driver()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Allow some time for the page to load completely\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Get the page source\n",
    "    page_content = driver.page_source\n",
    "\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "    container = soup.find('ul', class_ = \"startups-list\").find_all('li')\n",
    "\n",
    "    for el in container:\n",
    "        logo = \"https://www.ai-startups-europe.eu\" + el.find('img')['src']\n",
    "        name = el.find('h2').text\n",
    "        website = el.find('a', class_ = \"meta-item fw-bold stretched-link\")[\"href\"]\n",
    "\n",
    "        initial_info = {\"name\": name, \n",
    "                        \"logo\": logo,\n",
    "                        \"website_url\": website\n",
    "                        }\n",
    "\n",
    "\n",
    "        if website:                \n",
    "            startup_text = get_all_text(website)\n",
    "        else:\n",
    "            startup_text = \"\"\n",
    "\n",
    "        # Ottieni le notizie relative alla startup\n",
    "        news_summary = news([name]) \n",
    "            \n",
    "        # Concatena le informazioni per l'analisi GPT\n",
    "        full_text = f\"Startup Information:\\n{startup_text}\\n\\nNews Summary:\\n{news_summary}\"\n",
    "\n",
    "        # Analyze the content with GPT\n",
    "        analysis_result = analyze_text_with_gpt(full_text, initial_info)\n",
    "        data_dict = convert_to_dict(analysis_result)\n",
    "        \n",
    "        data_dict['google_news_urls'] = news_summary\n",
    "        data_dict['Key'] = generate_unique_key(website)\n",
    "\n",
    "        # Convert the dictionary into a DataFrame\n",
    "        df = pd.DataFrame([data_dict])\n",
    "\n",
    "        # Upload the first record in 'df' to Airtable\n",
    "        upload_record_to_airtable(df.iloc[0].to_dict(), url_airtable, headers)\n",
    "        print('upload_record_to_airtable Ã¨ andato OK')\n",
    "\n",
    "\n",
    "        break \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK INTEGRATED_TABLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Airtable API Key e Base ID\n",
    "api_key = 'patRHlt60PBL6jlV5.f2b9afed49bf23d9ab73cee6b283af76d3b4a9ebc0619002c6f104cc84382f2c'\n",
    "base_id = 'appUMz37tbS84AKX4'\n",
    "table_name = 'integrated_table'\n",
    "\n",
    "# Endpoint dell'API di Airtable\n",
    "url = f'https://api.airtable.com/v0/{base_id}/{table_name}'\n",
    "\n",
    "\n",
    "# Intestazioni per la richiesta API\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {api_key}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "def get_all_records(url, headers):\n",
    "    all_records = []\n",
    "    params = {\"pageSize\": 100}\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        \n",
    "        records = response.json().get('records', [])\n",
    "        all_records.extend(records)\n",
    "        \n",
    "        # Verifica se c'Ã¨ un'altra pagina di record\n",
    "        offset = response.json().get('offset')\n",
    "        if not offset:\n",
    "            break\n",
    "        params['offset'] = offset\n",
    "\n",
    "    return all_records\n",
    "\n",
    "# Ottieni tutti i record\n",
    "records = get_all_records(url, headers)\n",
    "\n",
    "# Converti i record in un DataFrame pandas per una visualizzazione migliore\n",
    "if records:\n",
    "    df = pd.DataFrame([record['fields'] for record in records])\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Nessun record trovato.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "# Define your connection parameters\n",
    "DB_HOST = '35.231.104.140'\n",
    "DB_PORT = '5432'  # Default PostgreSQL port\n",
    "DB_NAME = 'rh_ai_storage'\n",
    "DB_USER = 'kevin_capano'\n",
    "DB_PASSWORD = '56LXhzMhTa9a'\n",
    "\n",
    "# Function to establish a connection to the PostgreSQL database\n",
    "def connect_db():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT,\n",
    "            dbname=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD\n",
    "        )\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to connect to the database: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_filter_query(table_name, filters):\n",
    "    # Start building the query\n",
    "    query = sql.SQL(\"SELECT * FROM {table} WHERE \").format(\n",
    "        table=sql.Identifier(table_name)\n",
    "    )\n",
    "\n",
    "    filter_clauses = []\n",
    "    filter_values = []\n",
    "\n",
    "    for field, condition in filters.items():\n",
    "        if isinstance(condition, str) and any(op in condition for op in [\"<\", \">\", \"<=\", \">=\", \"!=\"]):\n",
    "            operator, value = condition.split(\" \", 1)\n",
    "            filter_clauses.append(sql.SQL(\"{field} {operator} %s\").format(\n",
    "                field=sql.Identifier(field),\n",
    "                operator=sql.SQL(operator)\n",
    "            ))\n",
    "            filter_values.append(value)\n",
    "        else:\n",
    "            filter_clauses.append(sql.SQL(\"{field} = %s\").format(\n",
    "                field=sql.Identifier(field)\n",
    "            ))\n",
    "            filter_values.append(condition)\n",
    "\n",
    "    query = query + sql.SQL(\" AND \").join(filter_clauses)\n",
    "\n",
    "    return query, filter_values\n",
    "\n",
    "def query_db(conn, table_name, filters):\n",
    "    query, filter_values = create_filter_query(table_name, filters)\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(query, filter_values)\n",
    "            results = cursor.fetchall()\n",
    "            return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def insert_record(conn, table_name, record):\n",
    "    fields = sql.SQL(', ').join(map(sql.Identifier, record.keys()))\n",
    "    values = sql.SQL(', ').join(sql.Placeholder() * len(record))\n",
    "\n",
    "    query = sql.SQL(\"INSERT INTO {table} ({fields}) VALUES ({values}) RETURNING id\").format(\n",
    "        table=sql.Identifier(table_name),\n",
    "        fields=fields,\n",
    "        values=values\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(query, list(record.values()))\n",
    "            conn.commit()\n",
    "            inserted_id = cursor.fetchone()[0]\n",
    "            return inserted_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting record: {e}\")\n",
    "        conn.rollback()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection\n",
    "conn = connect_db()\n",
    "\n",
    "# Example: Query the database with multiple filters\n",
    "filters = {\n",
    "    \"name\":'Nome Azienda'\n",
    "}\n",
    "\n",
    "results = query_db(conn, \"companies\", filters)\n",
    "\n",
    "if results:\n",
    "    print(\"Query Results:\")\n",
    "    for row in results:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection\n",
    "conn = connect_db()\n",
    "\n",
    "# Example: Insert a new record\n",
    "new_record = {\n",
    "    \"id\": 1,\n",
    "    \"name\": \"OpenAI\"\n",
    "}\n",
    "\n",
    "inserted_id = insert_record(conn, \"companies\", new_record)\n",
    "\n",
    "if inserted_id:\n",
    "    print(f\"New record inserted with ID: {inserted_id}\")\n",
    "\n",
    "# Close connection\n",
    "if conn:\n",
    "    conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b36e66cb547cdc54b4925ce66c24e32c3ebe3f01179b2b2245a66e674bcdf804"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
